{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Project Solution Lab 2: "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Problem: Recommend Movies or Shows to Users"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Modified from:\n", "- [Implementing a Recommender System with SageMaker, MXNet, and Gluon](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/gluon_recommender_system/gluon_recommender_system.ipynb)\n", "- [An Introduction to Factorization Machines with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/factorization_machines_mnist/factorization_machines_mnist.ipynb)\n", "- [Extending Amazon SageMaker Factorization Machines Algorithm to Predict Top X Recommendations](https://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction to business scenario\n", "\n", "You work for a startup that focuses on delivering on-demand video streaming services to users. The company wants to introduce movie/show recommendations for their users based on their viewing history.\n", "\n", "You are tasked with solving part of this problem by leveraging machine learning to create a recommendation engine to be used on the user website. You are given access to the dataset of historical user preferences and the movies they watched. You can use this to train a machine learning model to recommend movies/shows to watch.\n", "\n", "## About this dataset  \n", "The Amazon Customer Reviews Dataset is a collection of reviews on different products from the Amazon.com marketplace from 1995 until 2015. Customer reviews are one of the most important data types at Amazon. Collecting and showing reviews has been part of the Amazon culture since the beginning of the company and is arguably one important source of innovation. For more details on this dataset, see [Amazon Customer Reviews Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n", "\n", "This exercise focuses on reviews of videos. The videos dataset contains 1- to 5-star ratings from over 2M Amazon customers on 160K digital videos."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Features\n", "\n", "**Data columns**\n", "\n", "- `marketplace`: Two-letter country code (in this case, all \"US\")\n", "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author\n", "- `review_id`: Unique ID for the review\n", "- `product_id`: Amazon Standard Identification Number (ASIN). http://www.amazon.com/dp/<ASIN\\> links to the product's detail page.\n", "- `product_parent`: The parent of that ASIN. Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n", "- `product_title`: Title description of the product\n", "- `product_category`: Broad product category that can be used to group reviews (in this case, digital videos)\n", "- `star_rating`: Product's rating (1 to 5 stars)\n", "- `helpful_votes`: Number of helpful votes for the review\n", "- `total_votes`: Number of total votes the review received\n", "- `vine`: Was the review written as part of the Vine program?\n", "- `verified_purchase`: Was the review from a verified purchase?\n", "- `review_headline`: Title of the review itself\n", "- `review_body`: Text of the review\n", "- `review_date`: Date the review was written \n", "\n", "**Data format**\n", "- Tab `\\t` separated text file, without quote or escape characters\n", "- First line in each file is header; 1 line corresponds to 1 record\n", "\n", "### Dataset attributions\n", "\n", "Website: https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n", "\n", "This dataset is being provided to you by permission of Amazon and is subject to the terms of the AWS Digital Training Service Agreement (available at https://aws.amazon.com/training/digital-training-agreement). You are expressly prohibited from copying, modifying, selling, exporting, or using this dataset in any way other than for the purpose of completing this lab."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Brainstorming and designing a question...\n", "\n", "...That you can answer with machine learning. \n", "\n", "The first step in most projects is to think about the question you want to ask, how the data available supports this question, and which tool (in this case, machine learning model) you are going to use to answer the question. This is an important step because it helps narrow the scope of exploration and gives clarity on the features that you are going to use. \n", "\n", "Take a moment to write your thoughts regarding the dataset in the cell below. What are the things you can predict with machine learning? Why may that be relevant from a business/client perspective? Explain why you consider these thoughts important."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Write your thoughts here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There might be several ideas about what to do with the data, but for now we are all going to work on recommending a video to a particular user."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Recommendation and factorization machines\n", "\n", "In many ways, recommender systems were a catalyst for the current popularity of machine learning. One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature. The million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n", "\n", "Recommender systems can utilize a multitude of data sources and machine learning algorithms. Most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework. However, the core component is almost always a model that predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users. The minimal required dataset for this is a history of user item ratings (which we have).\n", "\n", "The method that you'll use is a factorization machine. A factorization machine is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model and is designed to parsimoniously (simply) capture interactions between features in high-dimensional sparse datasets. This makes it a good candidate to handle data patterns with features such as click prediction and item recommendation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 1: Problem formulation and data collection\n", "\n", "Start this project off by writing a few sentences below that summarize the business problem and the business goal you're trying to achieve in this scenario. Include a business metric you would like your team to aspire toward. With that information defined, clearly write out the machine learning problem statement. Finally, add a comment or two about the type of machine learning this represents.\n", "\n", "#### <span style=\"color: blue;\">Project presentation: Include a summary of these details in your project presentations.</span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Read through a business scenario and:\n", "\n", "### 1. Determine if and why ML is an appropriate solution to deploy.\n", "\\# Write your answer here\n", "\n", "### 2. Formulate the business problem, success metrics, and desired ML output.\n", "\\# Write your answer here\n", "\n", "### 3. Identify the type of ML problem you’re dealing with.\n", "\\# Write your answer here\n", "\n", "### 4. Analyze the appropriateness of the data you’re working with.\n", "\\# Write your answer here\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Setup\n", "\n", "Now that we have decided where to focus our energy, let's set things up so you can start working on solving the problem.\n", "\n", "**Note:** This notebook was created and tested on an `ml.m4.xlarge` notebook instance. \n", "\n", "Start by specifying:\n", "- The Amazon Simple Storage Service (Amazon S3) bucket and prefix(?) that you want to use for training and model data. This should be within the same Region as the Notebook Instance, training, and hosting.\n", "- The AWS Identity and Access Management (IAM) role [Amazon Resource Name (ARN)](https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html) used to give training and hosting access to your data. See the documentation for how to create these.\n", "\n", "**Note:** If more than one role is required for notebook instances, training, and/or hosting, replace the `get_execution_role()` call with the appropriate full IAM role ARN string(s)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Replace **`<LabBucketName>`** with the resource name that was provided with your lab account."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# Change the bucket and prefix according to your information\n", "#bucket = '<LabBucketName>'\n", "bucket = 'ml-pipeline-bucket'\n", "prefix = 'sagemaker-fm' \n", "\n", "import sagemaker\n", "role = sagemaker.get_execution_role()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, load some Python libraries you'll need for the remainder of this example notebook."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import os, subprocess\n", "import warnings\n", "import pandas as pd\n", "import numpy as np\n", "import sagemaker\n", "from sagemaker.mxnet import MXNet\n", "import boto3\n", "import json\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "# Add this to display all the outputs in the cell and not just the last one\n", "from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = \"all\"\n", "\n", "# Ignore warnings\n", "warnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 2: Data preprocessing and visualization  \n", "In this data preprocessing phase, you should take the opportunity to explore and visualize your data to better understand it. First, import the necessary libraries and read the data into a Pandas dataframe. After that, explore your data. Look for the shape of the dataset and explore your columns and the types of columns you're working with (numerical, categorical). Consider performing basic statistics on the features to get a sense of feature means and ranges. Take a close look at your target column and determine its distribution.\n", "\n", "### Specific questions to consider\n", "1. What can you deduce from the basic statistics you ran on the features? \n", "\n", "2. What can you deduce from the distributions of the target classes?\n", "\n", "3. Is there anything else you deduced from exploring the data?\n", "\n", "#### <span style=\"color: blue;\">Project presentation: Include a summary of your answers to these and other similar questions in your project presentations.</span>\n", "\n", "Start by bringing in the dataset from an Amazon S3 public bucket to this notebook environment."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": ["CompletedProcess(args=['mkdir', '-p', '/home/ec2-user/SageMaker/project/data/AmazonReviews'], returncode=0)"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": ["CompletedProcess(args=['aws', 's3', 'cp', 's3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', '/home/ec2-user/SageMaker/project/data/AmazonReviews'], returncode=0)"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["# Check whether the file is already in the desired path or if it needs to be downloaded\n", "\n", "base_path = '/home/ec2-user/SageMaker/project/data/AmazonReviews'\n", "file_path = '/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz'\n", "\n", "if not os.path.isfile(base_path + file_path):\n", "    subprocess.run(['mkdir', '-p', base_path])\n", "    subprocess.run(['aws', 's3', 'cp', 's3://amazon-reviews-pds/tsv' + file_path, base_path])\n", "else:\n", "    print('File already downloaded!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Reading the dataset\n", "\n", "Read the data into a Pandas dataframe so that you can know what you are dealing with.\n", "\n", "**Note:** You'll set `error_bad_lines=False` when reading the file in, because there appear to be a very small number of records that would create a problem otherwise.\n", "\n", "**Hint:** You can use the built-in Python `read_csv` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)). You can use the file path directly with Pandas `read_csv` with `delimiter='\\t'`.\n", "\n", "For example: `pd.read_csv('filename.tar.gz', delimiter = '\\t', error_bad_lines=False)`"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["b'Skipping line 92523: expected 15 fields, saw 22\\n'\n", "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n", "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n", "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n", "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n", "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n", "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n", "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n", "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n", "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"]}], "source": ["df = pd.read_csv(base_path + file_path, \n", "                 delimiter='\\t',\n", "                 error_bad_lines=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print the first few rows of your dataset.\n", "\n", "**Hint**: Use the `pandas.head(<number>)` function to print the rows."]}, {"cell_type": "code", "execution_count": 5, "metadata": {"scrolled": true}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>marketplace</th>\n", "      <th>customer_id</th>\n", "      <th>review_id</th>\n", "      <th>product_id</th>\n", "      <th>product_parent</th>\n", "      <th>product_title</th>\n", "      <th>product_category</th>\n", "      <th>star_rating</th>\n", "      <th>helpful_votes</th>\n", "      <th>total_votes</th>\n", "      <th>vine</th>\n", "      <th>verified_purchase</th>\n", "      <th>review_headline</th>\n", "      <th>review_body</th>\n", "      <th>review_date</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>US</td>\n", "      <td>12190288</td>\n", "      <td>R3FU16928EP5TC</td>\n", "      <td>B00AYB1482</td>\n", "      <td>668895143</td>\n", "      <td>Enlightened: Season 1</td>\n", "      <td>Digital_Video_Download</td>\n", "      <td>5</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>N</td>\n", "      <td>Y</td>\n", "      <td>I loved it and I wish there was a season 3</td>\n", "      <td>I loved it and I wish there was a season 3... ...</td>\n", "      <td>2015-08-31</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>US</td>\n", "      <td>30549954</td>\n", "      <td>R1IZHHS1MH3AQ4</td>\n", "      <td>B00KQD28OM</td>\n", "      <td>246219280</td>\n", "      <td>Vicious</td>\n", "      <td>Digital_Video_Download</td>\n", "      <td>5</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>N</td>\n", "      <td>Y</td>\n", "      <td>As always it seems that the best shows come fr...</td>\n", "      <td>As always it seems that the best shows come fr...</td>\n", "      <td>2015-08-31</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>US</td>\n", "      <td>52895410</td>\n", "      <td>R52R85WC6TIAH</td>\n", "      <td>B01489L5LQ</td>\n", "      <td>534732318</td>\n", "      <td>After Words</td>\n", "      <td>Digital_Video_Download</td>\n", "      <td>4</td>\n", "      <td>17</td>\n", "      <td>18</td>\n", "      <td>N</td>\n", "      <td>Y</td>\n", "      <td>Charming movie</td>\n", "      <td>This movie isn't perfect, but it gets a lot of...</td>\n", "      <td>2015-08-31</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["  marketplace  customer_id       review_id  product_id  product_parent  \\\n", "0          US     12190288  R3FU16928EP5TC  B00AYB1482       668895143   \n", "1          US     30549954  R1IZHHS1MH3AQ4  B00KQD28OM       246219280   \n", "2          US     52895410   R52R85WC6TIAH  B01489L5LQ       534732318   \n", "\n", "           product_title        product_category  star_rating  helpful_votes  \\\n", "0  Enlightened: Season 1  Digital_Video_Download            5              0   \n", "1                Vicious  Digital_Video_Download            5              0   \n", "2            After Words  Digital_Video_Download            4             17   \n", "\n", "   total_votes vine verified_purchase  \\\n", "0            0    N                 Y   \n", "1            0    N                 Y   \n", "2           18    N                 Y   \n", "\n", "                                     review_headline  \\\n", "0         I loved it and I wish there was a season 3   \n", "1  As always it seems that the best shows come fr...   \n", "2                                     Charming movie   \n", "\n", "                                         review_body review_date  \n", "0  I loved it and I wish there was a season 3... ...  2015-08-31  \n", "1  As always it seems that the best shows come fr...  2015-08-31  \n", "2  This movie isn't perfect, but it gets a lot of...  2015-08-31  "]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["df.head(3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now what is the information contained in all the columns?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Anatomy of the dataset\n", "\n", "Get a little more comfortable with the data and see what features are at hand.\n", "\n", "- `marketplace`: Two-letter country code (in this case, all \"US\")\n", "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author\n", "- `review_id`: Unique ID for the review\n", "- `product_id`: Amazon Standard Identification Number (ASIN). http://www.amazon.com/dp/<ASIN\\> links to the product's detail page.\n", "- `product_parent`: The parent of that ASIN. Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n", "- `product_title`: Title description of the product\n", "- `product_category`: Broad product category that can be used to group reviews (in this case, digital videos)\n", "- `star_rating`: Product's rating (1 to 5 stars)\n", "- `helpful_votes`: Number of helpful votes for the review\n", "- `total_votes`: Number of total votes the review received\n", "- `vine`: Was the review written as part of the Vine program?\n", "- `verified_purchase`: Was the review from a verified purchase?\n", "- `review_headline`: Title of the review itself\n", "- `review_body`: Text of the review\n", "- `review_date`: Date the review was written"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Analyzing and processing the dataset\n", "\n", "#### Exploring the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** How many rows and columns do you have in the dataset?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the size of the dataset.  \n", "\n", "**Hint**: Use the `<dataframe>.shape` function to check the size of your dataframe"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": ["(3998345, 15)"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["df.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Answer: (3998345,15)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** Which columns contain null values, and how many null values do they contain?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print a summary of the dataset.\n", "\n", "**Hint**: Use `<dataframe>.info` function using the keyword arguments `null_counts = True`"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class 'pandas.core.frame.DataFrame'>\n", "RangeIndex: 3998345 entries, 0 to 3998344\n", "Data columns (total 15 columns):\n", "marketplace          3998345 non-null object\n", "customer_id          3998345 non-null int64\n", "review_id            3998345 non-null object\n", "product_id           3998345 non-null object\n", "product_parent       3998345 non-null int64\n", "product_title        3998345 non-null object\n", "product_category     3998345 non-null object\n", "star_rating          3998345 non-null int64\n", "helpful_votes        3998345 non-null int64\n", "total_votes          3998345 non-null int64\n", "vine                 3998345 non-null object\n", "verified_purchase    3998345 non-null object\n", "review_headline      3998320 non-null object\n", "review_body          3998267 non-null object\n", "review_date          3998207 non-null object\n", "dtypes: int64(5), object(10)\n", "memory usage: 457.6+ MB\n"]}], "source": ["df.info(null_counts=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** Review headline: 25, Review_body: 78, Review_date: 138"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** Are there any duplicate rows? If yes, how many are there?\n", "\n", "**Hint**: Filter the dataframe using `dataframe.duplicated()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html#pandas.DataFrame.duplicated)) and check the length of the new dataframe."]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": ["0"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["duplicates = df[df.duplicated()]\n", "\n", "len(duplicates)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** There are no duplicated rows."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Data preprocessing\n", "\n", "Now it's time to decide what features you are going to use and how you are going to prepare them for your model. For this example, limit yourself to `customer_id`, `product_id`, `product_title`, and `star_rating`. Including additional features in the recommendation system could be beneficial but would require substantial processing (particularly the text data), which would be beyond the scope of this notebook.\n", "\n", "Reduce this dataset and only use the columns mentioned.\n", "\n", "**Hint:** Select multiple columns as a dataframe by passing the columns as a list. For example: `df[['column_name 1', 'column_name 2']]`"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["df_reduced = df[['customer_id', 'product_id', 'star_rating', 'product_title']]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check if you have duplicates after reducing the dataset. "]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": ["131"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["duplicates = df_reduced[df_reduced.duplicated()]\n", "\n", "len(duplicates)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer**: 131"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** Why do you have duplicates in your dataset now? What changed after you reduced the dataset? Review the first 20 lines of the duplicates.\n", "\n", "**Hint**: Use the `pandas.head(<number>)` function to print the rows."]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>customer_id</th>\n", "      <th>product_id</th>\n", "      <th>star_rating</th>\n", "      <th>product_title</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>565194</th>\n", "      <td>41454255</td>\n", "      <td>B00Y2UYRFS</td>\n", "      <td>1</td>\n", "      <td>unseen 2</td>\n", "    </tr>\n", "    <tr>\n", "      <th>594322</th>\n", "      <td>17570065</td>\n", "      <td>B00R3EEO2G</td>\n", "      <td>2</td>\n", "      <td>The Maze Runner</td>\n", "    </tr>\n", "    <tr>\n", "      <th>611264</th>\n", "      <td>15703996</td>\n", "      <td>B00I3MQNWG</td>\n", "      <td>5</td>\n", "      <td>Bosch Season 1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>612471</th>\n", "      <td>28456429</td>\n", "      <td>B008Y6W7J4</td>\n", "      <td>5</td>\n", "      <td>Rabbit Hole</td>\n", "    </tr>\n", "    <tr>\n", "      <th>613791</th>\n", "      <td>52388381</td>\n", "      <td>B00YORA25I</td>\n", "      <td>5</td>\n", "      <td>McFarland, USA (Theatrical)</td>\n", "    </tr>\n", "    <tr>\n", "      <th>685156</th>\n", "      <td>31828958</td>\n", "      <td>B00TT53YSW</td>\n", "      <td>5</td>\n", "      <td>Bates Motel, Season 3</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1204110</th>\n", "      <td>19462</td>\n", "      <td>B00QWUL4AW</td>\n", "      <td>5</td>\n", "      <td>Exodus: Gods and Kings</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1564816</th>\n", "      <td>24892653</td>\n", "      <td>B00L2GPYKW</td>\n", "      <td>5</td>\n", "      <td>The Escape Artist Season 1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1601662</th>\n", "      <td>44513234</td>\n", "      <td>B00NY4UIKG</td>\n", "      <td>5</td>\n", "      <td>The Equalizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1616429</th>\n", "      <td>43345475</td>\n", "      <td>B00P5968FC</td>\n", "      <td>3</td>\n", "      <td>The Babadook</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1616584</th>\n", "      <td>20779855</td>\n", "      <td>B00O99GXPE</td>\n", "      <td>5</td>\n", "      <td>Max Lucado's The Christmas Candle</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1617034</th>\n", "      <td>52861226</td>\n", "      <td>B00Q4FMCDS</td>\n", "      <td>2</td>\n", "      <td>The November Man</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1618497</th>\n", "      <td>735926</td>\n", "      <td>B00IQBE0BU</td>\n", "      <td>3</td>\n", "      <td>Once Upon A Forest</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1618598</th>\n", "      <td>3283400</td>\n", "      <td>B000NPQ0II</td>\n", "      <td>4</td>\n", "      <td>Creator</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1618822</th>\n", "      <td>50380731</td>\n", "      <td>B00C58TGQ4</td>\n", "      <td>3</td>\n", "      <td>Hemingway &amp; Gellhorn</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1618896</th>\n", "      <td>5465295</td>\n", "      <td>B009AP2B9Y</td>\n", "      <td>5</td>\n", "      <td>Kickin' It Volume 3</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1619053</th>\n", "      <td>39849456</td>\n", "      <td>B0035LN5YO</td>\n", "      <td>5</td>\n", "      <td>Gallipoli</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1620635</th>\n", "      <td>22158610</td>\n", "      <td>B00M0HXNPK</td>\n", "      <td>5</td>\n", "      <td>Transcendence (2014)</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1621358</th>\n", "      <td>46543213</td>\n", "      <td>B005OPTSIQ</td>\n", "      <td>5</td>\n", "      <td>Workaholics Season 2</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1621751</th>\n", "      <td>25315183</td>\n", "      <td>B00NEFWXNK</td>\n", "      <td>5</td>\n", "      <td>Arrow: Season 3</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["         customer_id  product_id  star_rating  \\\n", "565194      41454255  B00Y2UYRFS            1   \n", "594322      17570065  B00R3EEO2G            2   \n", "611264      15703996  B00I3MQNWG            5   \n", "612471      28456429  B008Y6W7J4            5   \n", "613791      52388381  B00YORA25I            5   \n", "685156      31828958  B00TT53YSW            5   \n", "1204110        19462  B00QWUL4AW            5   \n", "1564816     24892653  B00L2GPYKW            5   \n", "1601662     44513234  B00NY4UIKG            5   \n", "1616429     43345475  B00P5968FC            3   \n", "1616584     20779855  B00O99GXPE            5   \n", "1617034     52861226  B00Q4FMCDS            2   \n", "1618497       735926  B00IQBE0BU            3   \n", "1618598      3283400  B000NPQ0II            4   \n", "1618822     50380731  B00C58TGQ4            3   \n", "1618896      5465295  B009AP2B9Y            5   \n", "1619053     39849456  B0035LN5YO            5   \n", "1620635     22158610  B00M0HXNPK            5   \n", "1621358     46543213  B005OPTSIQ            5   \n", "1621751     25315183  B00NEFWXNK            5   \n", "\n", "                             product_title  \n", "565194                            unseen 2  \n", "594322                     The Maze Runner  \n", "611264                      Bosch Season 1  \n", "612471                         Rabbit Hole  \n", "613791         McFarland, USA (Theatrical)  \n", "685156               Bates Motel, Season 3  \n", "1204110             Exodus: Gods and Kings  \n", "1564816         The Escape Artist Season 1  \n", "1601662                      The Equalizer  \n", "1616429                       The Babadook  \n", "1616584  Max Lucado's The Christmas Candle  \n", "1617034                   The November Man  \n", "1618497                 Once Upon A Forest  \n", "1618598                            Creator  \n", "1618822               Hemingway & Gellhorn  \n", "1618896                Kickin' It Volume 3  \n", "1619053                          Gallipoli  \n", "1620635               Transcendence (2014)  \n", "1621358               Workaholics Season 2  \n", "1621751                    Arrow: Season 3  "]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["duplicates.head(20)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Hint:** Take a look at the first two elements in the duplicates dataframe, and query the original dataframe df to see what the data looks like. You can use the `query` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html)).\n", "\n", "For example:\n", "\n", "```\n", "df_eg = pd.DataFrame({\n", "            'A': [1,2,3,4],\n", "            'B': [\n", "        })\n", "df_eg.query('A > 1 & B > 0')\n", "```"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>marketplace</th>\n", "      <th>customer_id</th>\n", "      <th>review_id</th>\n", "      <th>product_id</th>\n", "      <th>product_parent</th>\n", "      <th>product_title</th>\n", "      <th>product_category</th>\n", "      <th>star_rating</th>\n", "      <th>helpful_votes</th>\n", "      <th>total_votes</th>\n", "      <th>vine</th>\n", "      <th>verified_purchase</th>\n", "      <th>review_headline</th>\n", "      <th>review_body</th>\n", "      <th>review_date</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>588857</th>\n", "      <td>US</td>\n", "      <td>17570065</td>\n", "      <td>RZ0MXSMTR2WQP</td>\n", "      <td>B00R3EEO2G</td>\n", "      <td>226574237</td>\n", "      <td>The Maze Runner</td>\n", "      <td>Digital_Video_Download</td>\n", "      <td>2</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>N</td>\n", "      <td>Y</td>\n", "      <td>Two Stars</td>\n", "      <td>Simply doesn't engage the audience.</td>\n", "      <td>2015-06-08</td>\n", "    </tr>\n", "    <tr>\n", "      <th>594322</th>\n", "      <td>US</td>\n", "      <td>17570065</td>\n", "      <td>R26IBV6N1BV1VN</td>\n", "      <td>B00R3EEO2G</td>\n", "      <td>226574237</td>\n", "      <td>The Maze Runner</td>\n", "      <td>Digital_Video_Download</td>\n", "      <td>2</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>N</td>\n", "      <td>Y</td>\n", "      <td>Two Stars</td>\n", "      <td>Simply doesn't engage the audience.</td>\n", "      <td>2015-06-08</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["       marketplace  customer_id       review_id  product_id  product_parent  \\\n", "588857          US     17570065   RZ0MXSMTR2WQP  B00R3EEO2G       226574237   \n", "594322          US     17570065  R26IBV6N1BV1VN  B00R3EEO2G       226574237   \n", "\n", "          product_title        product_category  star_rating  helpful_votes  \\\n", "588857  The Maze Runner  Digital_Video_Download            2              0   \n", "594322  The Maze Runner  Digital_Video_Download            2              0   \n", "\n", "        total_votes vine verified_purchase review_headline  \\\n", "588857            0    N                 Y       Two Stars   \n", "594322            0    N                 Y       Two Stars   \n", "\n", "                                review_body review_date  \n", "588857  Simply doesn't engage the audience.  2015-06-08  \n", "594322  Simply doesn't engage the audience.  2015-06-08  "]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["df.query(\"customer_id == 17570065 & product_id == 'B00R3EEO2G'\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** The dataset has duplicates because there are products with the same information but different `review_id` or `product_id`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Before continuing, remove the duplicate rows.\n", "\n", "**Hint**: Use the `~` operator to select all the rows that aren't duplicated. For example:\n", "    \n", "```\n", "df_eg = pd.DataFrame({\n", "            'A': [1,2,3,4],\n", "            'B': [2,0,5,2]\n", "        })\n", "df_eg[~(df_eg['B'] > 0)]\n", "```"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_reduced = df_reduced[~df_reduced.duplicated()]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualize some of the rows in the dataset\n", "If you haven't done so in the above, you can use the space below to further visualize some of your data. Look specifically at the distribution of features like `star_rating`, `customer_id`, and `product_id`.\n", "\n", "**Specific questions to consider**\n", "\n", "1. After looking at the distributions of features, to what extent might those features help your model? Is there anything you can deduce from those distributions that might be helpful in better understanding your data? \n", "\n", "2. Should you use all the data? What features should you use?\n", "\n", "3. What month has the highest count of user ratings?\n", "\n", "Use the cells below to visualize your data and answer these and other questions that might be of interest to you. Insert and delete cells where needed.\n", "\n", "#### <span style=\"color: blue;\">Project presentation: Include a summary of your answers to these and similar questions in your project presentations.</span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use `sns.barplot` ([documentation](https://seaborn.pydata.org/generated/seaborn.barplot.html)) to plot the `star_rating` density and distribution."]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>index</th>\n", "      <th>star_rating</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>5</td>\n", "      <td>2410533</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>4</td>\n", "      <td>756423</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>3</td>\n", "      <td>345891</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>1</td>\n", "      <td>289731</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2</td>\n", "      <td>195767</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   index  star_rating\n", "0      5      2410533\n", "1      4       756423\n", "2      3       345891\n", "3      1       289731\n", "4      2       195767"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": ["<matplotlib.axes._subplots.AxesSubplot at 0x7f021ff0f400>"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# Count the number of reviews with a specific rating\n", "df['star_rating'].value_counts().reset_index()\n", "sns.barplot(\n", "    x='index', \n", "    y='star_rating', \n", "    data=_,  # The underscore symbol in Python is used to store the output of the last operation\n", "    palette='GnBu_d'\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** What month contains the highest count of user ratings?\n", "\n", "**Hint**:  \n", "1. Use `pd.to_datetime` to convert the `review_date` column to a datetime column.  \n", "2. Use the month from the `review_date` column. You can access it for a datetime column using `<column_name>.dt.month`.\n", "3. Use the `groupby` function using `idxmax`.\n"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>review_date</th>\n", "      <th>star_rating</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1.0</td>\n", "      <td>309083</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2.0</td>\n", "      <td>369059</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>3.0</td>\n", "      <td>432084</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>4.0</td>\n", "      <td>363611</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>5.0</td>\n", "      <td>339285</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5</th>\n", "      <td>6.0</td>\n", "      <td>328114</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6</th>\n", "      <td>7.0</td>\n", "      <td>417844</td>\n", "    </tr>\n", "    <tr>\n", "      <th>7</th>\n", "      <td>8.0</td>\n", "      <td>464912</td>\n", "    </tr>\n", "    <tr>\n", "      <th>8</th>\n", "      <td>9.0</td>\n", "      <td>206453</td>\n", "    </tr>\n", "    <tr>\n", "      <th>9</th>\n", "      <td>10.0</td>\n", "      <td>218583</td>\n", "    </tr>\n", "    <tr>\n", "      <th>10</th>\n", "      <td>11.0</td>\n", "      <td>219316</td>\n", "    </tr>\n", "    <tr>\n", "      <th>11</th>\n", "      <td>12.0</td>\n", "      <td>329863</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["    review_date  star_rating\n", "0           1.0       309083\n", "1           2.0       369059\n", "2           3.0       432084\n", "3           4.0       363611\n", "4           5.0       339285\n", "5           6.0       328114\n", "6           7.0       417844\n", "7           8.0       464912\n", "8           9.0       206453\n", "9          10.0       218583\n", "10         11.0       219316\n", "11         12.0       329863"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": ["<matplotlib.axes._subplots.AxesSubplot at 0x7f022ffedb70>"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# Convert the review date to a datetime type and count the number of ratings by month\n", "df['review_date'] = pd.to_datetime(df.review_date)\n", "df.groupby(df.review_date.dt.month).star_rating.count().reset_index()\n", "sns.barplot(x='review_date', y='star_rating', data=_, palette='GnBu_d')"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The month with the most reviews is: 8.0\n"]}], "source": ["max_month = df.groupby(df.review_date.dt.month).star_rating.count().idxmax()\n", "print(f'The month with the most reviews is: {max_month}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Bonus question (optional):** Which years have the most and least reviews?"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>review_date</th>\n", "      <th>star_rating</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>2000.0</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2001.0</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2002.0</td>\n", "      <td>5</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2003.0</td>\n", "      <td>4</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2004.0</td>\n", "      <td>6</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5</th>\n", "      <td>2005.0</td>\n", "      <td>11</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6</th>\n", "      <td>2006.0</td>\n", "      <td>185</td>\n", "    </tr>\n", "    <tr>\n", "      <th>7</th>\n", "      <td>2007.0</td>\n", "      <td>2597</td>\n", "    </tr>\n", "    <tr>\n", "      <th>8</th>\n", "      <td>2008.0</td>\n", "      <td>3079</td>\n", "    </tr>\n", "    <tr>\n", "      <th>9</th>\n", "      <td>2009.0</td>\n", "      <td>3262</td>\n", "    </tr>\n", "    <tr>\n", "      <th>10</th>\n", "      <td>2010.0</td>\n", "      <td>6091</td>\n", "    </tr>\n", "    <tr>\n", "      <th>11</th>\n", "      <td>2011.0</td>\n", "      <td>20890</td>\n", "    </tr>\n", "    <tr>\n", "      <th>12</th>\n", "      <td>2012.0</td>\n", "      <td>142466</td>\n", "    </tr>\n", "    <tr>\n", "      <th>13</th>\n", "      <td>2013.0</td>\n", "      <td>721748</td>\n", "    </tr>\n", "    <tr>\n", "      <th>14</th>\n", "      <td>2014.0</td>\n", "      <td>1543049</td>\n", "    </tr>\n", "    <tr>\n", "      <th>15</th>\n", "      <td>2015.0</td>\n", "      <td>1554812</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["    review_date  star_rating\n", "0        2000.0            1\n", "1        2001.0            1\n", "2        2002.0            5\n", "3        2003.0            4\n", "4        2004.0            6\n", "5        2005.0           11\n", "6        2006.0          185\n", "7        2007.0         2597\n", "8        2008.0         3079\n", "9        2009.0         3262\n", "10       2010.0         6091\n", "11       2011.0        20890\n", "12       2012.0       142466\n", "13       2013.0       721748\n", "14       2014.0      1543049\n", "15       2015.0      1554812"]}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": ["<matplotlib.axes._subplots.AxesSubplot at 0x7f022ff884a8>"]}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 720x360 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["df.groupby(df.review_date.dt.year).star_rating.count().reset_index()\n", "fig = plt.gcf()\n", "fig.set_size_inches(10, 5)\n", "sns.barplot(x='review_date', y='star_rating', data=_, palette='GnBu_d')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** The years with the least amount of data are 2000 and 2001, with only 1 review. The year with the highest number of reviews is 2015."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Cleaning data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question**: How heterogeneous are the number of reviews per customer and reviews per video? Use quantiles to find out.\n", "\n", "**Hint**: Use `<dataframe>['columns_name'].value_counts()` for the customers and products dataframe, and use `<dataframe>.quantile(<list>)` to find the relationship."]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["customers\n", " 0.000       1.0\n", "0.010       1.0\n", "0.020       1.0\n", "0.030       1.0\n", "0.040       1.0\n", "0.050       1.0\n", "0.100       1.0\n", "0.250       1.0\n", "0.500       1.0\n", "0.750       2.0\n", "0.900       4.0\n", "0.950       5.0\n", "0.960       6.0\n", "0.970       7.0\n", "0.980       9.0\n", "0.990      13.0\n", "0.995      18.0\n", "0.999      37.0\n", "1.000    2704.0\n", "Name: customer_id, dtype: float64\n", "products\n", " 0.000        1.000\n", "0.010        1.000\n", "0.020        1.000\n", "0.030        1.000\n", "0.040        1.000\n", "0.050        1.000\n", "0.100        1.000\n", "0.250        1.000\n", "0.500        3.000\n", "0.750        9.000\n", "0.900       31.000\n", "0.950       73.000\n", "0.960       95.000\n", "0.970      130.000\n", "0.980      199.000\n", "0.990      386.670\n", "0.995      699.000\n", "0.999     1993.901\n", "1.000    32790.000\n", "Name: product_id, dtype: float64\n"]}], "source": ["customers = df['customer_id'].value_counts()\n", "products = df['product_id'].value_counts()\n", "\n", "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, \n", "             0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995, \n", "             0.999, 1]\n", "print('customers\\n', customers.quantile(quantiles))\n", "print('products\\n', products.quantile(quantiles))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** Only about 5% of customers have rated 5 or more videos, and only 25% of videos have been rated by 9+ customers."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Filter out this long tail. Select the customers that have rated 18 or more videos and the products that have more than 95 reviews."]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["customers1 = customers[customers >= 18]\n", "products1 = products[products >= 95]\n", "\n", "reduced_df = (\n", "    df_reduced.merge(pd.DataFrame({'customer_id': customers1.index}))\n", "              .merge(pd.DataFrame({'product_id': products1.index}))\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** What is the shape of `customers1`, `products1`, and the new dataframe reduced_df?\n", "\n", "**Note**: Use f-strings for this:\n", "\n", "```\n", "x= 3\n", "print(f'X = {x}')\n", "```"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Number of users is 10538 and number of items is 6683.\n", "Length of reduced df is 173506.\n"]}], "source": ["print(f'Number of users is {customers1.shape[0]} and number of items is {products1.shape[0]}.')\n", "print(f'Length of reduced df is {reduced_df.shape[0]}.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print the first 5 columns of the dataframe."]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>customer_id</th>\n", "      <th>product_id</th>\n", "      <th>star_rating</th>\n", "      <th>product_title</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>11763902</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>4</td>\n", "      <td>Downton Abbey Season 5</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>1411480</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>35303629</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>21285980</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>29260449</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   customer_id  product_id  star_rating           product_title\n", "0     11763902  B00PSLQYWE            4  Downton Abbey Season 5\n", "1      1411480  B00PSLQYWE            5  Downton Abbey Season 5\n", "2     35303629  B00PSLQYWE            5  Downton Abbey Season 5\n", "3     21285980  B00PSLQYWE            5  Downton Abbey Season 5\n", "4     29260449  B00PSLQYWE            5  Downton Abbey Season 5"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["reduced_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** Does `reduced_df` maintain the same ratio of ratings?"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>index</th>\n", "      <th>star_rating</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>5</td>\n", "      <td>86202</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>4</td>\n", "      <td>42669</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>3</td>\n", "      <td>24471</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2</td>\n", "      <td>10803</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>1</td>\n", "      <td>9361</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   index  star_rating\n", "0      5        86202\n", "1      4        42669\n", "2      3        24471\n", "3      2        10803\n", "4      1         9361"]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": ["<matplotlib.axes._subplots.AxesSubplot at 0x7f022fe2e048>"]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["reduced_df['star_rating'].value_counts().reset_index()\n", "sns.barplot(x='index', y='star_rating', data=_, palette='GnBu_d')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** The number of reviews with a 1-star rating decreased in proportion to 2-star ratings. That was not the case with the original data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, recreate the customer and product distributions of count per customer and product.  \n", "\n", "**Hint**: Use the `value_counts()` function on the `customer_id` and `product_id` columns."]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"data": {"text/plain": ["Text(0.5, 0.98, 'Distribution of counts per customer and product')"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": ["<matplotlib.axes._subplots.AxesSubplot at 0x7f022fc769e8>"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": ["<matplotlib.axes._subplots.AxesSubplot at 0x7f022fbea048>"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 1440x360 with 2 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["customers = reduced_df['customer_id'].value_counts()\n", "products = reduced_df['product_id'].value_counts()\n", "\n", "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n", "fig.suptitle('Distribution of counts per customer and product')\n", "sns.distplot(customers, kde=False, ax=axs[0], color='teal')\n", "sns.distplot(products, kde=False, ax=axs[1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, number each user and item, giving them their own sequential index. This will allow you to hold the information in a sparse format where the sequential indices indicate the row and column in the ratings matrix.\n", "\n", "To create the `customer_index` and `product_index`, create a new dataframe with `customer_id` as the index value and a sequential counter/values for the user and item number. Once you are finished creating both indexes, use the Pandas `merge` function to merge `customer_index` with `product_index`.\n", "\n", "**Hint**: Use the `shape` function to generate the total number of customers and products. Use `np.arange` to generate a list of numbers from 0 to the number of customers and products."]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>customer_id</th>\n", "      <th>product_id</th>\n", "      <th>star_rating</th>\n", "      <th>product_title</th>\n", "      <th>user</th>\n", "      <th>item</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>11763902</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>4</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>3065</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>1411480</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>130</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>35303629</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>4683</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>21285980</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>449</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>29260449</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>131</td>\n", "      <td>103</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   customer_id  product_id  star_rating           product_title  user  item\n", "0     11763902  B00PSLQYWE            4  Downton Abbey Season 5  3065   103\n", "1      1411480  B00PSLQYWE            5  Downton Abbey Season 5   130   103\n", "2     35303629  B00PSLQYWE            5  Downton Abbey Season 5  4683   103\n", "3     21285980  B00PSLQYWE            5  Downton Abbey Season 5   449   103\n", "4     29260449  B00PSLQYWE            5  Downton Abbey Season 5   131   103"]}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": ["customer_index = pd.DataFrame({'customer_id': customers.index, \n", "                               'user': np.arange(customers.shape[0])})\n", "product_index = pd.DataFrame({'product_id': products.index, \n", "                              'item': np.arange(products.shape[0])})\n", "\n", "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n", "reduced_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Sample answer:\n", "<div class=\"output_subarea\"><div>\n", "\n", "<table class=\"dataframe\" border=\"1\">\n", "  <thead>\n", "    <tr style=\"text-align: right\">\n", "      <th></th>\n", "      <th>customer_id</th>\n", "      <th>product_id</th>\n", "      <th>star_rating</th>\n", "      <th>product_title</th>\n", "      <th>user</th>\n", "      <th>item</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>11763902</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>4</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>3065</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>1411480</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>130</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>35303629</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>4683</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>21285980</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>449</td>\n", "      <td>103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>29260449</td>\n", "      <td>B00PSLQYWE</td>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 5</td>\n", "      <td>131</td>\n", "      <td>103</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div></div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Project Solution Lab 3: "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 3: Model training and evaluation\n", "\n", "There are some preliminary steps that you must include when converting the dataset from a dataframe to a format that a machine learning algorithm can use. For Amazon SageMaker, here are the steps you need to take:\n", "\n", "1. Split the data into `train_data` and `test_data`.    \n", "2. Convert the dataset to an appropriate file format that the Amazon SageMaker training job can use. This can be either a CSV file or record protobuf. For more information, see [Common Data Formats for Training](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html). For this problem, the data will be sparse, so you can use the `scipy.sparse.lilmatrix` function and then convert the function to the `RecordIO protobuf` format using `sagemaker.amazon.common.write_spmatrix_to_sparse_tensor`.    \n", "3. Upload the data to your Amazon S3 bucket. If you have not created one before, see [Create a Bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html).    \n", "\n", "Use the following cells to complete these steps. Insert and delete cells where needed.\n", "\n", "#### <span style=\"color: blue;\">Project presentation: Take note of the key decisions you've made in this phase in your project presentations.</span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Prepare the data\n", "\n", "You are at a point where you can start preparing the dataset as input for your model. Every model has different input needs. Some of the algorithms implemented in Amazon SageMaker require the data to be in the recordIO-wrapped protobuf form. You will take care of that in the following cells.\n", "\n", "First, split the dataset into training and test sets. This will allow you to estimate the model's accuracy on videos that customers rated but that weren't included in the training.\n", "\n", "Start with creation of the `test_df` dataframe. Create the dataframe by grouping the dataframe on `customer_id` and using the `last` function, similar to `pd.groupby('  ').last()`."]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": ["test_df = reduced_df.groupby('customer_id').last().reset_index()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To create the training data, remove the values present in `test_df` from the `reduced_df` dataframe.\n", "\n", "**Hint**: Merge the `reduced_df` dataframe with the `test_df` dataset with `customer_id` and `product_id` columns as an outer join."]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n", "                            on=['customer_id', 'product_id'], \n", "                            how='outer', \n", "                            indicator=True)\n", "train_df = train_df[(train_df['_merge'] == 'left_only')].reset_index()"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>customer_id</th>\n", "      <th>product_id</th>\n", "      <th>star_rating</th>\n", "      <th>product_title</th>\n", "      <th>user</th>\n", "      <th>item</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>10981</td>\n", "      <td>B00D6BQG1W</td>\n", "      <td>5</td>\n", "      <td>Warm Bodies</td>\n", "      <td>1859</td>\n", "      <td>3673</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>12783</td>\n", "      <td>B001GQK56G</td>\n", "      <td>4</td>\n", "      <td>Inspector Morse Season 1</td>\n", "      <td>1951</td>\n", "      <td>2794</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>17106</td>\n", "      <td>B00RPNBLWG</td>\n", "      <td>5</td>\n", "      <td>A Most Violent Year</td>\n", "      <td>10337</td>\n", "      <td>2057</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>19462</td>\n", "      <td>B00DFFI7K2</td>\n", "      <td>1</td>\n", "      <td>G.I. Joe: Retaliation</td>\n", "      <td>8674</td>\n", "      <td>624</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>54160</td>\n", "      <td>B005OPTSIQ</td>\n", "      <td>2</td>\n", "      <td>Workaholics Season 2</td>\n", "      <td>7309</td>\n", "      <td>3794</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   customer_id  product_id  star_rating             product_title   user  item\n", "0        10981  B00D6BQG1W            5               Warm Bodies   1859  3673\n", "1        12783  B001GQK56G            4  Inspector Morse Season 1   1951  2794\n", "2        17106  B00RPNBLWG            5       A Most Violent Year  10337  2057\n", "3        19462  B00DFFI7K2            1     G.I. Joe: Retaliation   8674   624\n", "4        54160  B005OPTSIQ            2      Workaholics Season 2   7309  3794"]}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": ["test_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now you can look at some basic characteristics of the data that will later help you convert the features to an appropriate format for training your model.\n", "\n", "Create two variables `nb_rating_test` and `nb_ratings_train` for the length of the test and training datasets."]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": [" Training Count:  163069\n", " Test Count:  10436\n"]}], "source": ["nb_ratings_test = len(test_df.index)\n", "nb_ratings_train = len(train_df.index)\n", "print(f\" Training Count: {nb_ratings_train}\")\n", "print(f\" Test Count: {nb_ratings_test}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Data conversion\n", "\n", "Now, you can convert your Pandas dataframes into a sparse matrix. This process is the same for both train and test. The Amazon SageMaker implementation of factorization machines takes recordIO-wrapped protobuf, where the data you have today is a Pandas dataframe on disk. Therefore, you are going to convert the data to a sparse matrix to express the relationships between each user and each movie."]}, {"cell_type": "code", "execution_count": 28, "metadata": {"scrolled": true}, "outputs": [], "source": ["# First convert data to sparse fomat with scipy lil_matrix\n", "\n", "from scipy.sparse import lil_matrix\n", "\n", "def loadDataset(df, lines, columns, regressor=True):\n", "    \"\"\"\n", "    Convert the pandas dataframe into sparse matrix\n", "    \n", "    Args:\n", "        df: DataFrame\n", "        lines: number of rows of the final sparse matrix\n", "        columns: number of columns of final sparse matrix\n", "        regressor: Boolean value to check if we are using regression\n", "                  or classification\n", "    Returns:\n", "        X: Feature vector\n", "        Y: Label vector\n", "    \"\"\"\n", "    # Features are one-hot encoded in a sparse matrix\n", "    # Use scipy.sparse.lil_matrix to create the feature vector X of type float32\n", "    X = lil_matrix((len(df), lines + columns)).astype('float32')\n", "    \n", "    # Labels are stored in a vector. Instantiate an empty label vector Y.\n", "    Y = []\n", "    \n", "    line = 0\n", "    \n", "    # For each row in the dataframe, use 1 for the item and product number\n", "    for index, row in df.iterrows():\n", "        X[line,row['user']] = 1\n", "        X[line, lines + (row['item'])] = 1\n", "        line += 1\n", "\n", "        if regressor:\n", "            # If using regression, append the star_rating\n", "            Y.append(row['star_rating'])\n", "        else:\n", "            # Use 1 for star_rating 5 else use 0\n", "            if int(row['star_rating']) >= 5:\n", "                Y.append(1)\n", "            else:\n", "                Y.append(0)\n", "    \n", "    # Convert the list into NumPy array of type float32  \n", "    Y = np.array(Y).astype('float32')\n", "    return X, Y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use the `loadDataset` function to create the training and test sets."]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["10436 6659 17095\n"]}], "source": ["print(customers.shape[0], \n", "      products.shape[0],\n", "      customers.shape[0] + products.shape[0])\n", "\n", "X_train, Y_train = loadDataset(train_df, customers.shape[0], \n", "                               products.shape[0])\n", "X_test, Y_test = loadDataset(test_df, customers.shape[0], \n", "                             products.shape[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that your data is in a sparse format, save it as a protobuf format and upload it to Amazon S3. This step might look intimidating, but most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as SageMaker below."]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Training data S3 path:  s3://ml-pipeline-010220085057-us-west-2-qls-150426-bf758deefff18ef6/sagemaker-fm/train\n", "Test data S3 path:  s3://ml-pipeline-010220085057-us-west-2-qls-150426-bf758deefff18ef6/sagemaker-fm/test\n"]}], "source": ["import io \n", "import sagemaker.amazon.common as smac\n", "\n", "def writeDatasetToProtobuf(X, bucket, prefix, key, d_type, Y=None):\n", "    buf = io.BytesIO()\n", "    if d_type == \"sparse\":\n", "        smac.write_spmatrix_to_sparse_tensor(buf, X, labels=Y)\n", "    else:\n", "        smac.write_numpy_to_dense_tensor(buf, X, labels=Y)\n", "        \n", "    buf.seek(0)\n", "    obj = '{}/{}'.format(prefix, key)\n", "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n", "    return 's3://{}/{}'.format(bucket,obj)\n", "    \n", "fm_train_data_path = writeDatasetToProtobuf(X_train, bucket, prefix, 'train', \"sparse\", Y_train)    \n", "fm_test_data_path  = writeDatasetToProtobuf(X_test, bucket, prefix, 'test', \"sparse\", Y_test)    \n", "  \n", "print(\"Training data S3 path: \", fm_train_data_path)\n", "print(\"Test data S3 path: \", fm_test_data_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You are finally finished with data preparation. Hooray! As you can see, it takes a lot of time and effort to clean and prepare the data for modeling. This is true for every single data science project, and this step has a high impact on the outcome. Make sure you spend enough time understanding and preparing your data for training in all future machine learning dventures!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training the model\n", "\n", "Now it's time to train the model. You will use an Amazon SageMaker training job for that. Amazon SageMaker training jobs are an easy way to create models, as you don't really have to write all the code for training. That is already handled for you in a nice container format.\n", "\n", "The general workflow for creating training jobs from the notebook is to instantiate the predictor, pass some hyperparameters, and then pass the data in the correct format. This is what happens in the following cell.\n", "\n", "For more more information about FM estimator, see [FactorizationMachines](https://sagemaker.readthedocs.io/en/stable/factorization_machines.html).\n", "\n", "For more information about hyperparameters, see [Factorization Machines Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html).\n", "\n", "**Hint**: Example:\n", "\n", "```\n", "sess = sagemaker.Session()\n", "\n", "pca = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n", "                                    role,\n", "                                    train_instance_count=1,\n", "                                    train_instance_type='ml.m4.xlarge',\n", "                                    output_path=output_location,\n", "                                    sagemaker_session=sess)\n", "                                    \n", "pca.set_hyperparameters(featuer_dim=50000,\n", "                        num_components=10,\n", "                        subtract_mean=True,\n", "                        algorithm_mode='randomized',\n", "                        mini_batch_size=200)\n", "                        \n", "pca.fit({'train': s3_train_data})\n", "```"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["2020-01-29 21:31:46 Starting - Starting the training job...\n", "2020-01-29 21:31:47 Starting - Launching requested ML instances...\n", "2020-01-29 21:32:45 Starting - Preparing the instances for training.........\n", "2020-01-29 21:34:12 Downloading - Downloading input data...\n", "2020-01-29 21:34:44 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n", "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n", "  from numpy.testing import nosetester\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'25', u'predictor_type': u'regressor', u'rescale_grad': u'0.0078125', u'clip_gradient': u'5.0', u'feature_dim': u'17095', u'num_factors': u'64', u'mini_batch_size': u'128'}\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'25', u'feature_dim': u'17095', u'num_factors': u'64', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'rescale_grad': u'0.0078125', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'128', u'_use_full_symbolic': u'true', u'clip_gradient': u'5.0', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 WARNING 139767193593664] Loggers have already been setup.\u001b[0m\n", "\u001b[34mProcess 1 is a worker.\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] Using default worker.\u001b[0m\n", "\u001b[34m[2020-01-29 21:34:47.123] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n", "\u001b[34m[2020-01-29 21:34:47.127] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 6, \"num_examples\": 1, \"num_bytes\": 8192}\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] nvidia-smi took: 0.0251798629761 secs to identify 0 gpus\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] Number of GPUs being used: 0\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] [Sparse network] Building a sparse network.\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] Create Store: local\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 48.477888107299805, \"sum\": 48.477888107299805, \"min\": 48.477888107299805}}, \"EndTime\": 1580333687.174169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333687.120396}\n", "\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1580333687.174362, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333687.174323}\n", "\u001b[0m\n", "\u001b[34m[21:34:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202222.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n", "\u001b[34m[21:34:47] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.202222.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=4.80276753591\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=23.066576004\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=4.77434062958\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:48 INFO 139767193593664] Iter[0] Batch [500]#011Speed: 51197.71 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:48 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=500 train rmse <loss>=1.3358705727\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:48 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=500 train mse <loss>=1.784550187\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:48 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=500 train absolute_loss <loss>=1.02346575712\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] Iter[0] Batch [1000]#011Speed: 39948.37 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=1000 train rmse <loss>=1.26025612401\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=1000 train mse <loss>=1.5882454981\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, batch=1000 train absolute_loss <loss>=0.985364523205\u001b[0m\n", "\u001b[34m[2020-01-29 21:34:50.896] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 3660, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.24930354169\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, train mse <loss>=1.56075933929\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=0.983929157351\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}, \"update.time\": {\"count\": 1, \"max\": 3722.8360176086426, \"sum\": 3722.8360176086426, \"min\": 3722.8360176086426}}, \"EndTime\": 1580333690.897434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333687.17425}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1275, \"sum\": 1275.0, \"min\": 1275}, \"Total Records Seen\": {\"count\": 1, \"max\": 163197, \"sum\": 163197.0, \"min\": 163197}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1580333690.897703, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1580333687.174561}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=43797.4533257 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=0.854896005307\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=0.73084717989\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.808736562729\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:52 INFO 139767193593664] Iter[1] Batch [500]#011Speed: 50622.22 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:52 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=500 train rmse <loss>=1.10766578301\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:52 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=500 train mse <loss>=1.22692348685\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:52 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=500 train absolute_loss <loss>=0.883357313817\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:53 INFO 139767193593664] Iter[1] Batch [1000]#011Speed: 52107.68 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=1000 train rmse <loss>=1.13428310876\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=1000 train mse <loss>=1.28659817082\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, batch=1000 train absolute_loss <loss>=0.908172382341\u001b[0m\n", "\u001b[34m[2020-01-29 21:34:54.016] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 3116, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.14581126539\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.31288345589\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.919643116475\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3118.7191009521484, \"sum\": 3118.7191009521484, \"min\": 3118.7191009521484}}, \"EndTime\": 1580333694.017426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333690.897541}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2549, \"sum\": 2549.0, \"min\": 2549}, \"Total Records Seen\": {\"count\": 1, \"max\": 326266, \"sum\": 326266.0, \"min\": 326266}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1580333694.017702, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1580333690.898663}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=52279.5936953 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=0.860604810734\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=0.740640640259\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.812555670738\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[34m[01/29/2020 21:34:55 INFO 139767193593664] Iter[2] Batch [500]#011Speed: 53904.64 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:55 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=500 train rmse <loss>=1.09045468208\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:55 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=500 train mse <loss>=1.18909141368\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:55 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=500 train absolute_loss <loss>=0.867777257741\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:56 INFO 139767193593664] Iter[2] Batch [1000]#011Speed: 55645.11 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=1000 train rmse <loss>=1.11662823249\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=1000 train mse <loss>=1.2468586096\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, batch=1000 train absolute_loss <loss>=0.892877708782\u001b[0m\n", "\u001b[34m[2020-01-29 21:34:57.025] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 3005, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.12807314125\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.27254901202\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.904497271336\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3007.7741146087646, \"sum\": 3007.7741146087646, \"min\": 3007.7741146087646}}, \"EndTime\": 1580333697.026361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333694.017534}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3823, \"sum\": 3823.0, \"min\": 3823}, \"Total Records Seen\": {\"count\": 1, \"max\": 489335, \"sum\": 489335.0, \"min\": 489335}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1580333697.026665, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1580333694.018546}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54206.7779281 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=0.856440510405\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=0.733490347862\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.806509554386\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:58 INFO 139767193593664] Iter[3] Batch [500]#011Speed: 54481.49 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:58 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=500 train rmse <loss>=1.07488745929\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:58 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=500 train mse <loss>=1.15538305014\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:58 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=500 train absolute_loss <loss>=0.853455000116\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:59 INFO 139767193593664] Iter[3] Batch [1000]#011Speed: 55304.99 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=1000 train rmse <loss>=1.10113821716\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=1000 train mse <loss>=1.2125053733\u001b[0m\n", "\u001b[34m[01/29/2020 21:34:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, batch=1000 train absolute_loss <loss>=0.879246239806\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:00.049] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 3019, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.11272670494\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.23816071989\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.891196490621\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3022.5918292999268, \"sum\": 3022.5918292999268, \"min\": 3022.5918292999268}}, \"EndTime\": 1580333700.050371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333697.026469}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5097, \"sum\": 5097.0, \"min\": 5097}, \"Total Records Seen\": {\"count\": 1, \"max\": 652404, \"sum\": 652404.0, \"min\": 652404}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1580333700.050626, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1580333697.027734}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=53941.989254 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=0.850809131512\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=0.723876178265\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.798756837845\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:01 INFO 139767193593664] Iter[4] Batch [500]#011Speed: 54005.39 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:01 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=500 train rmse <loss>=1.06155505469\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:01 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=500 train mse <loss>=1.12689913414\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:01 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=500 train absolute_loss <loss>=0.841134214056\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:02 INFO 139767193593664] Iter[4] Batch [1000]#011Speed: 54415.89 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=1000 train rmse <loss>=1.08789693495\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=1000 train mse <loss>=1.18351974107\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, batch=1000 train absolute_loss <loss>=0.867549148741\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:03.057] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 3004, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.09963196849\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.20919046613\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.879783608031\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3006.5410137176514, \"sum\": 3006.5410137176514, \"min\": 3006.5410137176514}}, \"EndTime\": 1580333703.058083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333700.050457}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6371, \"sum\": 6371.0, \"min\": 6371}, \"Total Records Seen\": {\"count\": 1, \"max\": 815473, \"sum\": 815473.0, \"min\": 815473}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1580333703.058386, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1580333700.051499}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54229.1656324 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=0.845222600927\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=0.714401245117\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:03 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.791339039803\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:04 INFO 139767193593664] Iter[5] Batch [500]#011Speed: 54593.91 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:04 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=500 train rmse <loss>=1.04999803447\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:04 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=500 train mse <loss>=1.10249587239\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:04 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=500 train absolute_loss <loss>=0.830431257358\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[34m[01/29/2020 21:35:05 INFO 139767193593664] Iter[5] Batch [1000]#011Speed: 52467.40 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:05 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=1000 train rmse <loss>=1.07639845246\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:05 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=1000 train mse <loss>=1.15863362845\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:05 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, batch=1000 train absolute_loss <loss>=0.857358045184\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:06.095] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 3034, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.08825031892\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.18428875663\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.869804790597\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3037.037134170532, \"sum\": 3037.037134170532, \"min\": 3037.037134170532}}, \"EndTime\": 1580333706.096387, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333703.058194}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7645, \"sum\": 7645.0, \"min\": 7645}, \"Total Records Seen\": {\"count\": 1, \"max\": 978542, \"sum\": 978542.0, \"min\": 978542}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1580333706.096647, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1580333703.059303}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=53685.4250881 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=0.839885515461\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=0.705407679081\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:06 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.784629642963\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:07 INFO 139767193593664] Iter[6] Batch [500]#011Speed: 54663.67 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:07 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=500 train rmse <loss>=1.03978962385\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:07 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=500 train mse <loss>=1.08116246186\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:07 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=500 train absolute_loss <loss>=0.820958722613\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:08 INFO 139767193593664] Iter[6] Batch [1000]#011Speed: 54985.03 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:08 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=1000 train rmse <loss>=1.06621972126\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:08 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=1000 train mse <loss>=1.136824494\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:08 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, batch=1000 train absolute_loss <loss>=0.848272358502\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:09.070] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2971, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.0781596306\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.16242818906\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.860877111016\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2973.747968673706, \"sum\": 2973.747968673706, \"min\": 2973.747968673706}}, \"EndTime\": 1580333709.071305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333706.096478}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8919, \"sum\": 8919.0, \"min\": 8919}, \"Total Records Seen\": {\"count\": 1, \"max\": 1141611, \"sum\": 1141611.0, \"min\": 1141611}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1580333709.07157, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1580333706.097519}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54827.5010704 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=0.834817439012\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=0.696920156479\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:09 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.778080284595\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:10 INFO 139767193593664] Iter[7] Batch [500]#011Speed: 53520.47 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:10 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=500 train rmse <loss>=1.03062517795\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:10 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=500 train mse <loss>=1.06218825742\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:10 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=500 train absolute_loss <loss>=0.812435279647\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:11 INFO 139767193593664] Iter[7] Batch [1000]#011Speed: 55869.09 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:11 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=1000 train rmse <loss>=1.05706329649\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:11 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=1000 train mse <loss>=1.11738281278\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:11 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, batch=1000 train absolute_loss <loss>=0.840037419216\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:12.066] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2991, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.06906754869\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.14290542367\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.852754951155\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2994.2431449890137, \"sum\": 2994.2431449890137, \"min\": 2994.2431449890137}}, \"EndTime\": 1580333712.066902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333709.0714}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10193, \"sum\": 10193.0, \"min\": 10193}, \"Total Records Seen\": {\"count\": 1, \"max\": 1304680, \"sum\": 1304680.0, \"min\": 1304680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1580333712.067166, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1580333709.072615}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54452.5683077 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=0.830005418174\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=0.688908994198\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:12 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.772132396698\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:13 INFO 139767193593664] Iter[8] Batch [500]#011Speed: 56094.19 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:13 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=500 train rmse <loss>=1.02229199025\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:13 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=500 train mse <loss>=1.04508091334\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:13 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=500 train absolute_loss <loss>=0.804665199178\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:14 INFO 139767193593664] Iter[8] Batch [1000]#011Speed: 54990.00 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:14 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=1000 train rmse <loss>=1.04872213298\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:14 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=1000 train mse <loss>=1.09981811219\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:14 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, batch=1000 train absolute_loss <loss>=0.832482881152\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:15.014] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2944, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.06077137254\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.1252359048\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.845279615402\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2946.8119144439697, \"sum\": 2946.8119144439697, \"min\": 2946.8119144439697}}, \"EndTime\": 1580333715.014873, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333712.066991}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11467, \"sum\": 11467.0, \"min\": 11467}, \"Total Records Seen\": {\"count\": 1, \"max\": 1467749, \"sum\": 1467749.0, \"min\": 1467749}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1580333715.015175, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1580333712.068019}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=55328.0793715 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=0.825428982668\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=0.681333005428\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:15 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.76632720232\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[34m[01/29/2020 21:35:16 INFO 139767193593664] Iter[9] Batch [500]#011Speed: 54226.62 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:16 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=500 train rmse <loss>=1.01463779098\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:16 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=500 train mse <loss>=1.02948984688\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:16 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=500 train absolute_loss <loss>=0.797497733208\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:17 INFO 139767193593664] Iter[9] Batch [1000]#011Speed: 54668.57 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:17 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=1000 train rmse <loss>=1.04104809143\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:17 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=1000 train mse <loss>=1.08378112867\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:17 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, batch=1000 train absolute_loss <loss>=0.825475757802\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:18.053] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 3035, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.05312622408\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.10907484385\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.838324727102\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3037.957191467285, \"sum\": 3037.957191467285, \"min\": 3037.957191467285}}, \"EndTime\": 1580333718.054102, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333715.014981}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12741, \"sum\": 12741.0, \"min\": 12741}, \"Total Records Seen\": {\"count\": 1, \"max\": 1630818, \"sum\": 1630818.0, \"min\": 1630818}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1580333718.05435, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1580333715.016107}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=53669.7709006 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=0.821063869234\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=0.674145877361\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:18 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=0.760772109032\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:19 INFO 139767193593664] Iter[10] Batch [500]#011Speed: 56040.22 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:19 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=500 train rmse <loss>=1.00755015877\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:19 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=500 train mse <loss>=1.01515732245\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:19 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=500 train absolute_loss <loss>=0.790817484111\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:20 INFO 139767193593664] Iter[10] Batch [1000]#011Speed: 51179.70 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:20 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=1000 train rmse <loss>=1.03393170954\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:20 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=1000 train mse <loss>=1.06901477999\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:20 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, batch=1000 train absolute_loss <loss>=0.818923288828\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:21.095] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 3038, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=1.04602506494\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, train mse <loss>=1.09416843649\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=0.831803844993\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 3040.7211780548096, \"sum\": 3040.7211780548096, \"min\": 3040.7211780548096}}, \"EndTime\": 1580333721.096077, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333718.054196}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14015, \"sum\": 14015.0, \"min\": 14015}, \"Total Records Seen\": {\"count\": 1, \"max\": 1793887, \"sum\": 1793887.0, \"min\": 1793887}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1580333721.096348, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 10}, \"StartTime\": 1580333718.055319}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=53620.7106746 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=0.816886395621\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=0.66730338335\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:21 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=0.755527675152\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:22 INFO 139767193593664] Iter[11] Batch [500]#011Speed: 55281.69 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:22 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=500 train rmse <loss>=1.00094322839\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:22 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=500 train mse <loss>=1.00188734646\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:22 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=500 train absolute_loss <loss>=0.784565031112\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:23 INFO 139767193593664] Iter[11] Batch [1000]#011Speed: 55211.16 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:23 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=1000 train rmse <loss>=1.02728934032\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:23 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=1000 train mse <loss>=1.05532338873\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:23 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, batch=1000 train absolute_loss <loss>=0.812762128574\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:24.049] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2950, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=1.0393861508\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, train mse <loss>=1.08032357047\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=0.825657040887\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2952.8729915618896, \"sum\": 2952.8729915618896, \"min\": 2952.8729915618896}}, \"EndTime\": 1580333724.050229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333721.096189}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 15289, \"sum\": 15289.0, \"min\": 15289}, \"Total Records Seen\": {\"count\": 1, \"max\": 1956956, \"sum\": 1956956.0, \"min\": 1956956}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1580333724.050474, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 11}, \"StartTime\": 1580333721.097318}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=55216.158896 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=0.812871664579\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=0.660760343075\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:24 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=0.750391066074\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:25 INFO 139767193593664] Iter[12] Batch [500]#011Speed: 55457.59 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:25 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=500 train rmse <loss>=0.994749624209\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:25 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=500 train mse <loss>=0.989526814865\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:25 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=500 train absolute_loss <loss>=0.778677395063\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[34m[01/29/2020 21:35:26 INFO 139767193593664] Iter[12] Batch [1000]#011Speed: 53437.85 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:26 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=1000 train rmse <loss>=1.02105531636\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:26 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=1000 train mse <loss>=1.04255395907\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:26 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, batch=1000 train absolute_loss <loss>=0.806938247545\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:27.043] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2990, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=1.03314536748\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, train mse <loss>=1.06738935034\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=0.819833173669\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2993.0620193481445, \"sum\": 2993.0620193481445, \"min\": 2993.0620193481445}}, \"EndTime\": 1580333727.044568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333724.050318}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16563, \"sum\": 16563.0, \"min\": 16563}, \"Total Records Seen\": {\"count\": 1, \"max\": 2120025, \"sum\": 2120025.0, \"min\": 2120025}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1580333727.044871, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 12}, \"StartTime\": 1580333724.051468}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54473.3199677 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=0.80899550183\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=0.654473721981\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:27 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=0.745404720306\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:28 INFO 139767193593664] Iter[13] Batch [500]#011Speed: 55318.48 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:28 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=500 train rmse <loss>=0.988915248585\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:28 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=500 train mse <loss>=0.977953368883\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:28 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=500 train absolute_loss <loss>=0.773108176783\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] Iter[13] Batch [1000]#011Speed: 56386.02 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=1000 train rmse <loss>=1.01517685408\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=1000 train mse <loss>=1.03058404506\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, batch=1000 train absolute_loss <loss>=0.801409541399\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:29.968] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2920, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=1.02725123603\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, train mse <loss>=1.05524510193\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=0.814292920277\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2923.41685295105, \"sum\": 2923.41685295105, \"min\": 2923.41685295105}}, \"EndTime\": 1580333729.969302, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333727.044681}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 17837, \"sum\": 17837.0, \"min\": 17837}, \"Total Records Seen\": {\"count\": 1, \"max\": 2283094, \"sum\": 2283094.0, \"min\": 2283094}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1580333729.969587, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 13}, \"StartTime\": 1580333727.045842}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=55771.0749621 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=0.805236110891\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=0.648405194283\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:29 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=0.74048936367\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:31 INFO 139767193593664] Iter[14] Batch [500]#011Speed: 53633.05 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:31 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=500 train rmse <loss>=0.983395793891\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:31 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=500 train mse <loss>=0.967067287443\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:31 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=500 train absolute_loss <loss>=0.767811413356\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] Iter[14] Batch [1000]#011Speed: 55811.99 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=1000 train rmse <loss>=1.00961071581\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=1000 train mse <loss>=1.01931379747\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, batch=1000 train absolute_loss <loss>=0.79613645537\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:32.941] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2969, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=1.02166164759\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, train mse <loss>=1.04379252216\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=0.809001099254\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2971.8339443206787, \"sum\": 2971.8339443206787, \"min\": 2971.8339443206787}}, \"EndTime\": 1580333732.942383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333729.969394}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19111, \"sum\": 19111.0, \"min\": 19111}, \"Total Records Seen\": {\"count\": 1, \"max\": 2446163, \"sum\": 2446163.0, \"min\": 2446163}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1580333732.942621, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 14}, \"StartTime\": 1580333729.970512}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54864.1188457 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=0.80157275277\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=0.642518877983\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:32 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=0.735634326935\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:34 INFO 139767193593664] Iter[15] Batch [500]#011Speed: 55776.78 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:34 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=500 train rmse <loss>=0.978154403926\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:34 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=500 train mse <loss>=0.956786037919\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:34 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=500 train absolute_loss <loss>=0.762765054336\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] Iter[15] Batch [1000]#011Speed: 55789.90 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=1000 train rmse <loss>=1.00432088186\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=1000 train mse <loss>=1.00866043374\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, batch=1000 train absolute_loss <loss>=0.791096718935\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[34m[2020-01-29 21:35:35.904] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2959, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=1.0163415585\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, train mse <loss>=1.03295016354\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=0.803936834987\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2961.7550373077393, \"sum\": 2961.7550373077393, \"min\": 2961.7550373077393}}, \"EndTime\": 1580333735.90539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333732.94247}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20385, \"sum\": 20385.0, \"min\": 20385}, \"Total Records Seen\": {\"count\": 1, \"max\": 2609232, \"sum\": 2609232.0, \"min\": 2609232}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1580333735.905657, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 15}, \"StartTime\": 1580333732.943592}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=55049.9642337 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=0.797988226216\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=0.636785209179\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:35 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=0.730830550194\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:37 INFO 139767193593664] Iter[16] Batch [500]#011Speed: 53214.18 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:37 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=500 train rmse <loss>=0.973160131264\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:37 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=500 train mse <loss>=0.947040641082\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:37 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=500 train absolute_loss <loss>=0.757945802992\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] Iter[16] Batch [1000]#011Speed: 56016.57 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=1000 train rmse <loss>=0.999277055513\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=1000 train mse <loss>=0.998554633675\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, batch=1000 train absolute_loss <loss>=0.786271448557\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:38.886] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 2978, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=1.01126152771\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, train mse <loss>=1.02264987743\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=0.79908054398\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2980.520009994507, \"sum\": 2980.520009994507, \"min\": 2980.520009994507}}, \"EndTime\": 1580333738.887095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333735.905475}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 21659, \"sum\": 21659.0, \"min\": 21659}, \"Total Records Seen\": {\"count\": 1, \"max\": 2772301, \"sum\": 2772301.0, \"min\": 2772301}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1580333738.887339, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 16}, \"StartTime\": 1580333735.906537}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54704.2553523 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=0.794466567806\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=0.631177127361\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:38 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=0.726068735123\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:40 INFO 139767193593664] Iter[17] Batch [500]#011Speed: 55469.17 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:40 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=500 train rmse <loss>=0.968386620142\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:40 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=500 train mse <loss>=0.93777264607\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:40 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=500 train absolute_loss <loss>=0.753324841965\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] Iter[17] Batch [1000]#011Speed: 54382.23 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=1000 train rmse <loss>=0.994453399856\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=1000 train mse <loss>=0.988937564484\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, batch=1000 train absolute_loss <loss>=0.781634176885\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:41.865] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2975, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=1.00639646988\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, train mse <loss>=1.01283385458\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=0.79440708166\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2978.299140930176, \"sum\": 2978.299140930176, \"min\": 2978.299140930176}}, \"EndTime\": 1580333741.866703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333738.887185}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22933, \"sum\": 22933.0, \"min\": 22933}, \"Total Records Seen\": {\"count\": 1, \"max\": 2935370, \"sum\": 2935370.0, \"min\": 2935370}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1580333741.866972, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 17}, \"StartTime\": 1580333738.888364}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54743.9947033 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=0.790994865239\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=0.625672876835\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:41 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=0.721341371536\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:43 INFO 139767193593664] Iter[18] Batch [500]#011Speed: 55123.09 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:43 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=500 train rmse <loss>=0.963811252752\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:43 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=500 train mse <loss>=0.928932130932\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:43 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=500 train absolute_loss <loss>=0.748882725032\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] Iter[18] Batch [1000]#011Speed: 55836.92 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=1000 train rmse <loss>=0.989827698305\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=1000 train mse <loss>=0.979758872331\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, batch=1000 train absolute_loss <loss>=0.777170394952\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:44.790] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2921, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=1.00172483429\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, train mse <loss>=1.00345264362\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=0.789902927526\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2923.5799312591553, \"sum\": 2923.5799312591553, \"min\": 2923.5799312591553}}, \"EndTime\": 1580333744.791506, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333741.866791}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24207, \"sum\": 24207.0, \"min\": 24207}, \"Total Records Seen\": {\"count\": 1, \"max\": 3098439, \"sum\": 3098439.0, \"min\": 3098439}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1580333744.791785, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 18}, \"StartTime\": 1580333741.86789}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=55768.4238101 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=0.787563221528\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=0.620255827904\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:44 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=0.716643154621\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[34m[01/29/2020 21:35:45 INFO 139767193593664] Iter[19] Batch [500]#011Speed: 54835.56 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:45 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=500 train rmse <loss>=0.959414490797\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:45 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=500 train mse <loss>=0.920476165152\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:45 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=500 train absolute_loss <loss>=0.744607951946\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] Iter[19] Batch [1000]#011Speed: 55638.77 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=1000 train rmse <loss>=0.985380732217\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=1000 train mse <loss>=0.970975187424\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, batch=1000 train absolute_loss <loss>=0.77286731908\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:47.773] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 2979, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=0.99722798286\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, train mse <loss>=0.994463649799\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=0.785555508742\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2981.8508625030518, \"sum\": 2981.8508625030518, \"min\": 2981.8508625030518}}, \"EndTime\": 1580333747.774608, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333744.791598}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 25481, \"sum\": 25481.0, \"min\": 25481}, \"Total Records Seen\": {\"count\": 1, \"max\": 3261508, \"sum\": 3261508.0, \"min\": 3261508}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1580333747.774893, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 19}, \"StartTime\": 1580333744.792713}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54678.3350533 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=0 train rmse <loss>=0.784164561581\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=0 train mse <loss>=0.614914059639\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:47 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=0 train absolute_loss <loss>=0.711970925331\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:48 INFO 139767193593664] Iter[20] Batch [500]#011Speed: 55572.38 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:48 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=500 train rmse <loss>=0.955179313637\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:48 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=500 train mse <loss>=0.9123675212\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:48 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=500 train absolute_loss <loss>=0.740485719756\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] Iter[20] Batch [1000]#011Speed: 56692.46 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=1000 train rmse <loss>=0.981095738838\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=1000 train mse <loss>=0.962548848766\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, batch=1000 train absolute_loss <loss>=0.768713475018\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:50.763] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 2986, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, train rmse <loss>=0.992889677239\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, train mse <loss>=0.985829911167\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=20, train absolute_loss <loss>=0.781352586073\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2988.637924194336, \"sum\": 2988.637924194336, \"min\": 2988.637924194336}}, \"EndTime\": 1580333750.764488, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333747.774699}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 26755, \"sum\": 26755.0, \"min\": 26755}, \"Total Records Seen\": {\"count\": 1, \"max\": 3424577, \"sum\": 3424577.0, \"min\": 3424577}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1580333750.764783, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 20}, \"StartTime\": 1580333747.775805}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54553.8915118 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=0 train rmse <loss>=0.780792522002\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=0 train mse <loss>=0.609636962414\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:50 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=0 train absolute_loss <loss>=0.707335233688\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:51 INFO 139767193593664] Iter[21] Batch [500]#011Speed: 55166.66 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:51 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=500 train rmse <loss>=0.951090753247\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:51 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=500 train mse <loss>=0.904573620912\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:51 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=500 train absolute_loss <loss>=0.736508682043\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] Iter[21] Batch [1000]#011Speed: 55025.04 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=1000 train rmse <loss>=0.976957998547\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=1000 train mse <loss>=0.954446930926\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, batch=1000 train absolute_loss <loss>=0.764700521122\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:53.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 2967, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, train rmse <loss>=0.988695655924\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, train mse <loss>=0.977519100044\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=21, train absolute_loss <loss>=0.777286382635\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2969.9389934539795, \"sum\": 2969.9389934539795, \"min\": 2969.9389934539795}}, \"EndTime\": 1580333753.735701, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333750.764587}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 28029, \"sum\": 28029.0, \"min\": 28029}, \"Total Records Seen\": {\"count\": 1, \"max\": 3587646, \"sum\": 3587646.0, \"min\": 3587646}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 23, \"sum\": 23.0, \"min\": 23}}, \"EndTime\": 1580333753.735991, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 21}, \"StartTime\": 1580333750.76572}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54897.7318057 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=0 train rmse <loss>=0.777444190816\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=0 train mse <loss>=0.604419469833\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:53 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=0 train absolute_loss <loss>=0.702729403973\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:54 INFO 139767193593664] Iter[22] Batch [500]#011Speed: 55638.44 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=500 train rmse <loss>=0.947135697848\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=500 train mse <loss>=0.897066030138\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:54 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=500 train absolute_loss <loss>=0.732661398109\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] Iter[22] Batch [1000]#011Speed: 54239.86 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=1000 train rmse <loss>=0.97295456747\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=1000 train mse <loss>=0.94664059036\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, batch=1000 train absolute_loss <loss>=0.760813476739\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:56.714] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 2976, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, train rmse <loss>=0.984633375638\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, train mse <loss>=0.969502884421\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=22, train absolute_loss <loss>=0.773342813671\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2978.5730838775635, \"sum\": 2978.5730838775635, \"min\": 2978.5730838775635}}, \"EndTime\": 1580333756.715567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333753.735798}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 29303, \"sum\": 29303.0, \"min\": 29303}, \"Total Records Seen\": {\"count\": 1, \"max\": 3750715, \"sum\": 3750715.0, \"min\": 3750715}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1580333756.71589, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 22}, \"StartTime\": 1580333753.736951}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54737.9486388 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=0 train rmse <loss>=0.774117488764\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=0 train mse <loss>=0.59925788641\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:56 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=0 train absolute_loss <loss>=0.698144316673\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:57 INFO 139767193593664] Iter[23] Batch [500]#011Speed: 54338.64 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=500 train rmse <loss>=0.943302465494\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=500 train mse <loss>=0.889819541407\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:57 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=500 train absolute_loss <loss>=0.728936104004\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] Iter[23] Batch [1000]#011Speed: 54143.96 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=1000 train rmse <loss>=0.969073944647\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=1000 train mse <loss>=0.939104310193\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, batch=1000 train absolute_loss <loss>=0.757045767941\u001b[0m\n", "\u001b[34m[2020-01-29 21:35:59.712] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 2994, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, train rmse <loss>=0.980691698917\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, train mse <loss>=0.961756208324\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=23, train absolute_loss <loss>=0.769515491215\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2996.608018875122, \"sum\": 2996.608018875122, \"min\": 2996.608018875122}}, \"EndTime\": 1580333759.713485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333756.71567}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30577, \"sum\": 30577.0, \"min\": 30577}, \"Total Records Seen\": {\"count\": 1, \"max\": 3913784, \"sum\": 3913784.0, \"min\": 3913784}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}}, \"EndTime\": 1580333759.713769, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 23}, \"StartTime\": 1580333756.716835}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54409.5844434 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=0 train rmse <loss>=0.770811265337\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=0 train mse <loss>=0.594150006771\u001b[0m\n", "\u001b[34m[01/29/2020 21:35:59 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=0 train absolute_loss <loss>=0.693581461906\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:00 INFO 139767193593664] Iter[24] Batch [500]#011Speed: 54201.46 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=500 train rmse <loss>=0.939580691678\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=500 train mse <loss>=0.882811876174\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:00 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=500 train absolute_loss <loss>=0.725324497846\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] Iter[24] Batch [1000]#011Speed: 54684.08 samples/sec\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=1000 train rmse <loss>=0.965305918194\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=1000 train mse <loss>=0.931815515701\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, batch=1000 train absolute_loss <loss>=0.753388893711\u001b[0m\n", "\u001b[34m[2020-01-29 21:36:02.696] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 2980, \"num_examples\": 1274, \"num_bytes\": 10436416}\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, train rmse <loss>=0.976860722619\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, train mse <loss>=0.954256871396\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, epoch=24, train absolute_loss <loss>=0.765798434451\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, train rmse <loss>=0.976860722619\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, train mse <loss>=0.954256871396\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #quality_metric: host=algo-1, train absolute_loss <loss>=0.765798434451\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2983.189105987549, \"sum\": 2983.189105987549, \"min\": 2983.189105987549}}, \"EndTime\": 1580333762.697881, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333759.713583}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1274, \"sum\": 1274.0, \"min\": 1274}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Total Batches Seen\": {\"count\": 1, \"max\": 31851, \"sum\": 31851.0, \"min\": 31851}, \"Total Records Seen\": {\"count\": 1, \"max\": 4076853, \"sum\": 4076853.0, \"min\": 4076853}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 163069, \"sum\": 163069.0, \"min\": 163069}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1580333762.698185, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 24}, \"StartTime\": 1580333759.714651}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] #throughput_metric: host=algo-1, train throughput=54653.6490046 records/second\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 WARNING 139767193593664] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] Pulling entire model from kvstore to finalize\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 4.657983779907227, \"sum\": 4.657983779907227, \"min\": 4.657983779907227}}, \"EndTime\": 1580333762.703246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333762.697983}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:02 INFO 139767193593664] Saved checkpoint to \"/tmp/tmpPU6oj8/state-0001.params\"\u001b[0m\n", "\u001b[34m[2020-01-29 21:36:02.729] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 75605, \"num_examples\": 1, \"num_bytes\": 8192}\u001b[0m\n", "\u001b[34m[2020-01-29 21:36:03.102] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 372, \"num_examples\": 82, \"num_bytes\": 667904}\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 10436, \"sum\": 10436.0, \"min\": 10436}, \"Total Batches Seen\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}, \"Total Records Seen\": {\"count\": 1, \"max\": 10436, \"sum\": 10436.0, \"min\": 10436}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 10436, \"sum\": 10436.0, \"min\": 10436}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1580333763.102957, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333762.729372}\n", "\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:03 INFO 139767193593664] #test_score (algo-1) : ('rmse', 1.1147151011937058)\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:03 INFO 139767193593664] #test_score (algo-1) : ('mse', 1.2425897568292936)\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:03 INFO 139767193593664] #test_score (algo-1) : ('absolute_loss', 0.8755719677247779)\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:03 INFO 139767193593664] #quality_metric: host=algo-1, test rmse <loss>=1.11471510119\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:03 INFO 139767193593664] #quality_metric: host=algo-1, test mse <loss>=1.24258975683\u001b[0m\n", "\u001b[34m[01/29/2020 21:36:03 INFO 139767193593664] #quality_metric: host=algo-1, test absolute_loss <loss>=0.875571967725\u001b[0m\n", "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 76033.15901756287, \"sum\": 76033.15901756287, \"min\": 76033.15901756287}, \"setuptime\": {\"count\": 1, \"max\": 43.78795623779297, \"sum\": 43.78795623779297, \"min\": 43.78795623779297}}, \"EndTime\": 1580333763.104196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1580333762.703733}\n", "\u001b[0m\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\n", "2020-01-29 21:36:13 Uploading - Uploading generated training model\n", "2020-01-29 21:36:13 Completed - Training job completed\n", "Training seconds: 121\n", "Billable seconds: 121\n"]}], "source": ["from sagemaker import get_execution_role\n", "from sagemaker.amazon.amazon_estimator import get_image_uri\n", "\n", "output_prefix = 's3://' + bucket + '/sagemaker-fm/model'\n", "instance_type='ml.m4.xlarge'\n", "batch_size = 128\n", "\n", "fm = sagemaker.estimator.Estimator(\n", "    get_image_uri(boto3.Session().region_name, \"factorization-machines\"),\n", "    role, \n", "    train_instance_count=1, \n", "    train_instance_type=instance_type,\n", "    output_path=output_prefix,\n", "    sagemaker_session=sagemaker.Session()\n", ")\n", "\n", "fm.set_hyperparameters(feature_dim=X_train.shape[1],\n", "                       predictor_type='regressor',\n", "                     # predictor_type='binary_classifier',\n", "                       mini_batch_size=batch_size,\n", "                       num_factors=64,\n", "                       epochs=25,\n", "                       clip_gradient=5.0,\n", "                       rescale_grad=1.0/batch_size)\n", "\n", "fm.fit({'train': fm_train_data_path, 'test': fm_test_data_path})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** What does changing the `batch_size` and `epochs` do to the final metric?  \n", "\n", "**Answer:** There are a couple of things to notice here. The `predictor_type` is set to `'regressor'` because you are trying to predict the star rating. The `batch_size` is set to 128. This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases. `num_factors` is set to 64. As mentioned initially, factorization machines find a lower dimensional representation of the interactions for all features. Making this value smaller provides a more parsimonious (simple) model, closer to a linear model, but may sacrifice information about interactions. Making it larger provides a higher dimensional representation of feature interactions but adds computational complexity and can lead to overfitting. In a practical application, time should be invested to tune this parameter to the appropriate value."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** Check the output of the model. What is the meaning of the metrics used? Is there a difference between the training and testing sets? If yes, what is the meaning of that?  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** The **MSE**, **RMSE**, and **Absolute loss** are considerably lower in the training set than in the testing dataset. This might indicate that the model is being molded to the pattern of the training set and not the general pattern on the reviews. This phenomenon is called overfitting."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Evaluate\n", "\n", "Congratulations! You have successfully launched an Amazon SageMaker training job. Now what? Well, you need a way to verify that your model is actually predicting coherent values. How do you do this?\n", "\n", "Start by calculating a naive baseline to approximate how well your model is doing. The simplest estimate would be to assume every user item rating is just the average rating over all ratings. This is basically saying that you have a model that only learned to output the mean value of all reviews.\n", "\n", "**Note:** You could do better by using each individual video's average; however, in this case, it doesn't really matter because the same conclusions would hold."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate the mean of `star_rating` to get the `naive_guess`. Then, calculate the naive MSE by squaring the naive guess from the test `star_rating` and getting an average.\n", "\n", "$average(test(star\\_rating) - naive\\_guess)^2)$"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Naive MSE: 1.501868207928254\n"]}], "source": ["naive_guess = np.mean(train_df['star_rating'])\n", "print('Naive MSE:', np.mean((test_df['star_rating'] - naive_guess) ** 2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, calculate predictions for your test dataset. To this end, you'll need to _deploy_ the model you just trained.\n", "\n", "**Note:** This will align closely to your CloudWatch output above but may differ slightly due to skipping partial mini-batches in the `eval_net` function.\n", "\n", "Use `<estimator_name>.deploy` with `initial_instance_count=1, instance_type=ml.m4.xlarge`."]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["---------------!"]}], "source": ["fm_predictor = fm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that your endpoint is 'InService', evaluate how your model performs on the test set. Compare that test set performance to the performance on the training set. \n", "\n", "### Key questions to consider:\n", "1. How does your model's performance on the test set compare to the training set? What can you deduce from this comparison? \n", "\n", "2. Are there obvious differences between the outcomes of metrics like accuracy, precision, and recall? If so, why might you be seeing those differences? \n", "\n", "3. Given your business situation and goals, which metric(s) is most important for you to consider here? Why?\n", "\n", "4. Is the outcome for the metric(s) you consider most important sufficient for what you need from a business standpoint? If not, what are some things you might change in your next iteration (in the feature engineering section, which is coming up next)? \n", "\n", "Use the cells below to answer these and other questions. Insert and delete cells where needed.\n", "\n", "#### <span style=\"color: blue;\">Project presentation: Record questions to these and other similar questions you might answer in this section in your project presentations. Record key details and decisions you've made in your project presentations.</span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The deployment process involves creating an instance of the specified size, in this case `ml.m4.xlarge`, with the model you trained and saved on Amazon S3. To get a prediction, you need to pass your data in a serialized form of JSON. The output you get from the inference will be in serialized JSON form as well, so you also need to deserialize it to get the predicted values."]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["# Create a serializer function for the predictor\n", "import json\n", "from sagemaker.predictor import json_deserializer\n", "\n", "def fm_serializer(data):\n", "    js = {'instances': []}\n", "    for row in data:\n", "        js['instances'].append({'features': row.tolist()})\n", "    return json.dumps(js)\n", "\n", "fm_predictor.content_type = 'application/json'\n", "fm_predictor.serializer = fm_serializer\n", "fm_predictor.deserializer = json_deserializer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check how your training set did. Use the endpoint to get predictions from your model.\n", "\n", "First, look at what a single prediction looks like."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Amazon SageMaker model containers must respond to requests within 60 seconds. The model itself can have a maximum processing time of 60 seconds before responding to the /invocations. To do that, call the `predict` function for 5 rows at a time and then add those rows to a list. "]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["# Pass the X_train data to the predictor deployed \n", "ytrain_p = []\n", "for i in range(0, 1000, 5):\n", "    preds = fm_predictor.predict(X_train[i:i + 5].toarray())['predictions']\n", "    p = [ytrain_p.append(x['score']) for x in preds]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** Now that you have inferences, do a sanity check. What are the minimum and maximum values predicted in the inferences? Do those correspond to the minimum and maximum values in the training data?"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The minimum rating predicted is:  3.165067195892334 and the maximum is:  4.865056037902832\n"]}], "source": ["print('The minimum rating predicted is: ', min(ytrain_p), 'and the maximum is: ',max(ytrain_p))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, check your test dataset."]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["Y_pred = []\n", "for i in range(0, X_test.shape[0], 5):\n", "    preds = fm_predictor.predict(X_test[i:i+5].toarray())['predictions']\n", "    p = [Y_pred.append(x['score']) for x in preds]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** How are the min and max values alike in the predictions? Bonus point if you check the entire distribution (histogram)."]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"data": {"text/plain": ["(5.0329670906066895, 2.6423165798187256)"]}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": ["max(Y_pred), min(Y_pred)"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"data": {"text/plain": ["<matplotlib.axes._subplots.AxesSubplot at 0x7f01c92a1860>"]}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["sns.distplot(Y_pred, kde=False, bins=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, calculate the mean squared error for the test set and see how much of an improvement it is from the baseline."]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["MSE: 1.242589756902862\n"]}], "source": ["print('MSE:', np.mean((test_df['star_rating'] - Y_pred) ** 2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For recommender systems, subjective accuracy also matters. Get some recommendations for a random user to see if they make intuitive sense.\n", "\n", "Try using user number 200, and see what they have watched and rated highly."]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>customer_id</th>\n", "      <th>product_id</th>\n", "      <th>star_rating</th>\n", "      <th>product_title</th>\n", "      <th>user</th>\n", "      <th>item</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>60487</th>\n", "      <td>43080952</td>\n", "      <td>B00DTOYJJ2</td>\n", "      <td>5</td>\n", "      <td>Breaking Bad The Final Season</td>\n", "      <td>200</td>\n", "      <td>76</td>\n", "    </tr>\n", "    <tr>\n", "      <th>14249</th>\n", "      <td>43080952</td>\n", "      <td>B00VFTA70I</td>\n", "      <td>5</td>\n", "      <td>The Imitation Game</td>\n", "      <td>200</td>\n", "      <td>88</td>\n", "    </tr>\n", "    <tr>\n", "      <th>28407</th>\n", "      <td>43080952</td>\n", "      <td>B00KG2NGPS</td>\n", "      <td>5</td>\n", "      <td>The Monuments Men</td>\n", "      <td>200</td>\n", "      <td>152</td>\n", "    </tr>\n", "    <tr>\n", "      <th>71980</th>\n", "      <td>43080952</td>\n", "      <td>B00TPJHW7Q</td>\n", "      <td>5</td>\n", "      <td>The Theory of Everything</td>\n", "      <td>200</td>\n", "      <td>155</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6753</th>\n", "      <td>43080952</td>\n", "      <td>B00IIVJU3Q</td>\n", "      <td>5</td>\n", "      <td>Philomena</td>\n", "      <td>200</td>\n", "      <td>180</td>\n", "    </tr>\n", "    <tr>\n", "      <th>84422</th>\n", "      <td>43080952</td>\n", "      <td>B0047WJ12K</td>\n", "      <td>5</td>\n", "      <td>Inception</td>\n", "      <td>200</td>\n", "      <td>473</td>\n", "    </tr>\n", "    <tr>\n", "      <th>161389</th>\n", "      <td>43080952</td>\n", "      <td>B00DGNO3WE</td>\n", "      <td>5</td>\n", "      <td>Quartet</td>\n", "      <td>200</td>\n", "      <td>1113</td>\n", "    </tr>\n", "    <tr>\n", "      <th>119444</th>\n", "      <td>43080952</td>\n", "      <td>B0059R5Q08</td>\n", "      <td>5</td>\n", "      <td>Jane Eyre</td>\n", "      <td>200</td>\n", "      <td>2711</td>\n", "    </tr>\n", "    <tr>\n", "      <th>167600</th>\n", "      <td>43080952</td>\n", "      <td>B001VLKWUA</td>\n", "      <td>5</td>\n", "      <td>Out of Africa</td>\n", "      <td>200</td>\n", "      <td>3064</td>\n", "    </tr>\n", "    <tr>\n", "      <th>136601</th>\n", "      <td>43080952</td>\n", "      <td>B00TF87WQI</td>\n", "      <td>5</td>\n", "      <td>Rosewater</td>\n", "      <td>200</td>\n", "      <td>3487</td>\n", "    </tr>\n", "    <tr>\n", "      <th>118793</th>\n", "      <td>43080952</td>\n", "      <td>B001P4NZ9I</td>\n", "      <td>5</td>\n", "      <td>Louis CK: Chewed Up</td>\n", "      <td>200</td>\n", "      <td>3703</td>\n", "    </tr>\n", "    <tr>\n", "      <th>99778</th>\n", "      <td>43080952</td>\n", "      <td>B00SH00N3Y</td>\n", "      <td>4</td>\n", "      <td>Lucy</td>\n", "      <td>200</td>\n", "      <td>26</td>\n", "    </tr>\n", "    <tr>\n", "      <th>40897</th>\n", "      <td>43080952</td>\n", "      <td>B00I8H6MGS</td>\n", "      <td>4</td>\n", "      <td>Dallas Buyers Club</td>\n", "      <td>200</td>\n", "      <td>40</td>\n", "    </tr>\n", "    <tr>\n", "      <th>102552</th>\n", "      <td>43080952</td>\n", "      <td>B00S65TBHY</td>\n", "      <td>4</td>\n", "      <td>Gone Girl</td>\n", "      <td>200</td>\n", "      <td>41</td>\n", "    </tr>\n", "    <tr>\n", "      <th>40443</th>\n", "      <td>43080952</td>\n", "      <td>B00J8CG6KE</td>\n", "      <td>4</td>\n", "      <td>Wolf Of Wall Street</td>\n", "      <td>200</td>\n", "      <td>46</td>\n", "    </tr>\n", "    <tr>\n", "      <th>27118</th>\n", "      <td>43080952</td>\n", "      <td>B00TPJE3ZK</td>\n", "      <td>4</td>\n", "      <td>Birdman</td>\n", "      <td>200</td>\n", "      <td>71</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3368</th>\n", "      <td>43080952</td>\n", "      <td>B00XVPL8G6</td>\n", "      <td>4</td>\n", "      <td>American Sniper</td>\n", "      <td>200</td>\n", "      <td>98</td>\n", "    </tr>\n", "    <tr>\n", "      <th>45654</th>\n", "      <td>43080952</td>\n", "      <td>B00J2PEU3W</td>\n", "      <td>4</td>\n", "      <td>Saving Mr. Banks</td>\n", "      <td>200</td>\n", "      <td>112</td>\n", "    </tr>\n", "    <tr>\n", "      <th>28279</th>\n", "      <td>43080952</td>\n", "      <td>B00IMY9TNK</td>\n", "      <td>4</td>\n", "      <td>Nebraska</td>\n", "      <td>200</td>\n", "      <td>195</td>\n", "    </tr>\n", "    <tr>\n", "      <th>93388</th>\n", "      <td>43080952</td>\n", "      <td>B00TF874XE</td>\n", "      <td>4</td>\n", "      <td>Nightcrawler</td>\n", "      <td>200</td>\n", "      <td>277</td>\n", "    </tr>\n", "    <tr>\n", "      <th>34018</th>\n", "      <td>43080952</td>\n", "      <td>B00RNRD8RA</td>\n", "      <td>4</td>\n", "      <td>The Interview</td>\n", "      <td>200</td>\n", "      <td>366</td>\n", "    </tr>\n", "    <tr>\n", "      <th>20026</th>\n", "      <td>43080952</td>\n", "      <td>B00XJDYITO</td>\n", "      <td>4</td>\n", "      <td>Still Alice</td>\n", "      <td>200</td>\n", "      <td>381</td>\n", "    </tr>\n", "    <tr>\n", "      <th>12546</th>\n", "      <td>43080952</td>\n", "      <td>B006IGZWB2</td>\n", "      <td>4</td>\n", "      <td>The Help</td>\n", "      <td>200</td>\n", "      <td>457</td>\n", "    </tr>\n", "    <tr>\n", "      <th>123749</th>\n", "      <td>43080952</td>\n", "      <td>B001Q2J8BS</td>\n", "      <td>4</td>\n", "      <td>Burn After Reading</td>\n", "      <td>200</td>\n", "      <td>3198</td>\n", "    </tr>\n", "    <tr>\n", "      <th>26130</th>\n", "      <td>43080952</td>\n", "      <td>B00797MB0O</td>\n", "      <td>4</td>\n", "      <td>Young Adult</td>\n", "      <td>200</td>\n", "      <td>3582</td>\n", "    </tr>\n", "    <tr>\n", "      <th>150899</th>\n", "      <td>43080952</td>\n", "      <td>B001AITGWE</td>\n", "      <td>4</td>\n", "      <td>Falling Down</td>\n", "      <td>200</td>\n", "      <td>5031</td>\n", "    </tr>\n", "    <tr>\n", "      <th>28625</th>\n", "      <td>43080952</td>\n", "      <td>B00IMYSCR4</td>\n", "      <td>3</td>\n", "      <td>Gravity</td>\n", "      <td>200</td>\n", "      <td>15</td>\n", "    </tr>\n", "    <tr>\n", "      <th>55117</th>\n", "      <td>43080952</td>\n", "      <td>B00HYTSTNU</td>\n", "      <td>3</td>\n", "      <td>Captain Phillips</td>\n", "      <td>200</td>\n", "      <td>47</td>\n", "    </tr>\n", "    <tr>\n", "      <th>142143</th>\n", "      <td>43080952</td>\n", "      <td>B00HZ4ASEC</td>\n", "      <td>3</td>\n", "      <td>Last Vegas</td>\n", "      <td>200</td>\n", "      <td>107</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6913</th>\n", "      <td>43080952</td>\n", "      <td>B00JP5ONY0</td>\n", "      <td>3</td>\n", "      <td>The Secret Life of Walter Mitty</td>\n", "      <td>200</td>\n", "      <td>117</td>\n", "    </tr>\n", "    <tr>\n", "      <th>112565</th>\n", "      <td>43080952</td>\n", "      <td>B00J2PRBNS</td>\n", "      <td>3</td>\n", "      <td>American Hustle</td>\n", "      <td>200</td>\n", "      <td>137</td>\n", "    </tr>\n", "    <tr>\n", "      <th>56581</th>\n", "      <td>43080952</td>\n", "      <td>B00QGKVB4W</td>\n", "      <td>3</td>\n", "      <td>The Hundred-Foot Journey (Theatrical)</td>\n", "      <td>200</td>\n", "      <td>198</td>\n", "    </tr>\n", "    <tr>\n", "      <th>50081</th>\n", "      <td>43080952</td>\n", "      <td>B00JJH3HCS</td>\n", "      <td>3</td>\n", "      <td>August: Osage County</td>\n", "      <td>200</td>\n", "      <td>206</td>\n", "    </tr>\n", "    <tr>\n", "      <th>74852</th>\n", "      <td>43080952</td>\n", "      <td>B00H8C4HZ2</td>\n", "      <td>3</td>\n", "      <td>Lee Daniels' The Butler</td>\n", "      <td>200</td>\n", "      <td>228</td>\n", "    </tr>\n", "    <tr>\n", "      <th>40084</th>\n", "      <td>43080952</td>\n", "      <td>B00L23RCPU</td>\n", "      <td>3</td>\n", "      <td>The Grand Budapest Hotel</td>\n", "      <td>200</td>\n", "      <td>231</td>\n", "    </tr>\n", "    <tr>\n", "      <th>97044</th>\n", "      <td>43080952</td>\n", "      <td>B00HAQAQME</td>\n", "      <td>3</td>\n", "      <td>Blue Jasmine</td>\n", "      <td>200</td>\n", "      <td>268</td>\n", "    </tr>\n", "    <tr>\n", "      <th>45571</th>\n", "      <td>43080952</td>\n", "      <td>B00VFTBPSG</td>\n", "      <td>3</td>\n", "      <td>Wild</td>\n", "      <td>200</td>\n", "      <td>349</td>\n", "    </tr>\n", "    <tr>\n", "      <th>43350</th>\n", "      <td>43080952</td>\n", "      <td>B00HNBS8MQ</td>\n", "      <td>3</td>\n", "      <td>Thanks For Sharing</td>\n", "      <td>200</td>\n", "      <td>710</td>\n", "    </tr>\n", "    <tr>\n", "      <th>121653</th>\n", "      <td>43080952</td>\n", "      <td>B00WG1FMCW</td>\n", "      <td>3</td>\n", "      <td>Cake</td>\n", "      <td>200</td>\n", "      <td>1215</td>\n", "    </tr>\n", "    <tr>\n", "      <th>155570</th>\n", "      <td>43080952</td>\n", "      <td>B00G32OMF8</td>\n", "      <td>3</td>\n", "      <td>Before Midnight</td>\n", "      <td>200</td>\n", "      <td>2560</td>\n", "    </tr>\n", "    <tr>\n", "      <th>74335</th>\n", "      <td>43080952</td>\n", "      <td>B00ST1OBS4</td>\n", "      <td>2</td>\n", "      <td>The Judge (2014)</td>\n", "      <td>200</td>\n", "      <td>33</td>\n", "    </tr>\n", "    <tr>\n", "      <th>97171</th>\n", "      <td>43080952</td>\n", "      <td>B00RVC3YKI</td>\n", "      <td>2</td>\n", "      <td>Boyhood</td>\n", "      <td>200</td>\n", "      <td>556</td>\n", "    </tr>\n", "    <tr>\n", "      <th>72806</th>\n", "      <td>43080952</td>\n", "      <td>B00U7B21A4</td>\n", "      <td>2</td>\n", "      <td>Foxcatcher</td>\n", "      <td>200</td>\n", "      <td>877</td>\n", "    </tr>\n", "    <tr>\n", "      <th>57297</th>\n", "      <td>43080952</td>\n", "      <td>B001DM1VXE</td>\n", "      <td>2</td>\n", "      <td>Pi</td>\n", "      <td>200</td>\n", "      <td>1794</td>\n", "    </tr>\n", "    <tr>\n", "      <th>29821</th>\n", "      <td>43080952</td>\n", "      <td>B005KL3R6I</td>\n", "      <td>1</td>\n", "      <td>Bridesmaids</td>\n", "      <td>200</td>\n", "      <td>557</td>\n", "    </tr>\n", "    <tr>\n", "      <th>115525</th>\n", "      <td>43080952</td>\n", "      <td>B001H6BIOS</td>\n", "      <td>1</td>\n", "      <td>Then She Found Me</td>\n", "      <td>200</td>\n", "      <td>1753</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["        customer_id  product_id  star_rating  \\\n", "60487      43080952  B00DTOYJJ2            5   \n", "14249      43080952  B00VFTA70I            5   \n", "28407      43080952  B00KG2NGPS            5   \n", "71980      43080952  B00TPJHW7Q            5   \n", "6753       43080952  B00IIVJU3Q            5   \n", "84422      43080952  B0047WJ12K            5   \n", "161389     43080952  B00DGNO3WE            5   \n", "119444     43080952  B0059R5Q08            5   \n", "167600     43080952  B001VLKWUA            5   \n", "136601     43080952  B00TF87WQI            5   \n", "118793     43080952  B001P4NZ9I            5   \n", "99778      43080952  B00SH00N3Y            4   \n", "40897      43080952  B00I8H6MGS            4   \n", "102552     43080952  B00S65TBHY            4   \n", "40443      43080952  B00J8CG6KE            4   \n", "27118      43080952  B00TPJE3ZK            4   \n", "3368       43080952  B00XVPL8G6            4   \n", "45654      43080952  B00J2PEU3W            4   \n", "28279      43080952  B00IMY9TNK            4   \n", "93388      43080952  B00TF874XE            4   \n", "34018      43080952  B00RNRD8RA            4   \n", "20026      43080952  B00XJDYITO            4   \n", "12546      43080952  B006IGZWB2            4   \n", "123749     43080952  B001Q2J8BS            4   \n", "26130      43080952  B00797MB0O            4   \n", "150899     43080952  B001AITGWE            4   \n", "28625      43080952  B00IMYSCR4            3   \n", "55117      43080952  B00HYTSTNU            3   \n", "142143     43080952  B00HZ4ASEC            3   \n", "6913       43080952  B00JP5ONY0            3   \n", "112565     43080952  B00J2PRBNS            3   \n", "56581      43080952  B00QGKVB4W            3   \n", "50081      43080952  B00JJH3HCS            3   \n", "74852      43080952  B00H8C4HZ2            3   \n", "40084      43080952  B00L23RCPU            3   \n", "97044      43080952  B00HAQAQME            3   \n", "45571      43080952  B00VFTBPSG            3   \n", "43350      43080952  B00HNBS8MQ            3   \n", "121653     43080952  B00WG1FMCW            3   \n", "155570     43080952  B00G32OMF8            3   \n", "74335      43080952  B00ST1OBS4            2   \n", "97171      43080952  B00RVC3YKI            2   \n", "72806      43080952  B00U7B21A4            2   \n", "57297      43080952  B001DM1VXE            2   \n", "29821      43080952  B005KL3R6I            1   \n", "115525     43080952  B001H6BIOS            1   \n", "\n", "                                product_title  user  item  \n", "60487           Breaking Bad The Final Season   200    76  \n", "14249                      The Imitation Game   200    88  \n", "28407                       The Monuments Men   200   152  \n", "71980                The Theory of Everything   200   155  \n", "6753                                Philomena   200   180  \n", "84422                               Inception   200   473  \n", "161389                                Quartet   200  1113  \n", "119444                              Jane Eyre   200  2711  \n", "167600                          Out of Africa   200  3064  \n", "136601                              Rosewater   200  3487  \n", "118793                    Louis CK: Chewed Up   200  3703  \n", "99778                                    Lucy   200    26  \n", "40897                      Dallas Buyers Club   200    40  \n", "102552                              Gone Girl   200    41  \n", "40443                     Wolf Of Wall Street   200    46  \n", "27118                                 Birdman   200    71  \n", "3368                          American Sniper   200    98  \n", "45654                        Saving Mr. Banks   200   112  \n", "28279                                Nebraska   200   195  \n", "93388                            Nightcrawler   200   277  \n", "34018                           The Interview   200   366  \n", "20026                             Still Alice   200   381  \n", "12546                                The Help   200   457  \n", "123749                     Burn After Reading   200  3198  \n", "26130                             Young Adult   200  3582  \n", "150899                           Falling Down   200  5031  \n", "28625                                 Gravity   200    15  \n", "55117                        Captain Phillips   200    47  \n", "142143                             Last Vegas   200   107  \n", "6913          The Secret Life of Walter Mitty   200   117  \n", "112565                        American Hustle   200   137  \n", "56581   The Hundred-Foot Journey (Theatrical)   200   198  \n", "50081                    August: Osage County   200   206  \n", "74852                 Lee Daniels' The Butler   200   228  \n", "40084                The Grand Budapest Hotel   200   231  \n", "97044                            Blue Jasmine   200   268  \n", "45571                                    Wild   200   349  \n", "43350                      Thanks For Sharing   200   710  \n", "121653                                   Cake   200  1215  \n", "155570                        Before Midnight   200  2560  \n", "74335                        The Judge (2014)   200    33  \n", "97171                                 Boyhood   200   556  \n", "72806                              Foxcatcher   200   877  \n", "57297                                      Pi   200  1794  \n", "29821                             Bridesmaids   200   557  \n", "115525                      Then She Found Me   200  1753  "]}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": ["reduced_df[reduced_df['user'] == 200].sort_values(\n", "    ['star_rating', 'item'], ascending=[False, True])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, this user likes to watch comedies, romance, and light-hearted movies and dislikes drama and fantasy movies. Let's see how your model predicts movie ratings for this user."]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": ["def prepare_predictions(user_id, number_movies, columns):\n", "    # Create the sparse matrix \n", "    X = lil_matrix((number_movies, columns)).astype('float32')\n", "    movie_index_start = columns - number_movies\n", "\n", "    # Fill out the matrix. Each row will be the same user with every possible movie.\n", "    for row in range(number_movies):\n", "        X[row, user_id - 1] = 1\n", "        X[row, movie_index_start + row] = 1\n", "\n", "    return X\n", "\n", "user_200 = prepare_predictions(200, products.shape[0], customers.shape[0] + products.shape[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now create a list of all the ratings that the model would predict for user 200 for all movies."]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [], "source": ["pred_200 = []\n", "for i in range(0, user_200.shape[0], 5):\n", "    preds = fm_predictor.predict(user_200[i:i+5].toarray())['predictions']\n", "    p = [pred_200.append(x['score']) for x in preds]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now loop through and predict user 200's ratings for every common video in the catalog to see which ones to recommend or not recommend. \n", "\n", "Create a new dataframe `titles` by using the `reduced_df` dataframe to group by the items. Use the `product_title` column and create another column `score` and add the values from `pred_200` to it."]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": ["titles = reduced_df.groupby('item')['product_title'].first().reset_index()\n", "titles['score'] = pred_200"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** What products got the highest score?  \n", "\n", "**Hint**: Use the `sort_values` function to sort columns `score` and `item` and use parameter `asecnding=[False,True]`"]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>item</th>\n", "      <th>product_title</th>\n", "      <th>score</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>25</th>\n", "      <td>25</td>\n", "      <td>John Adams Season 1</td>\n", "      <td>4.073676</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5</th>\n", "      <td>5</td>\n", "      <td>Downton Abbey Season 3</td>\n", "      <td>4.051294</td>\n", "    </tr>\n", "    <tr>\n", "      <th>12</th>\n", "      <td>12</td>\n", "      <td>Justified Season 3</td>\n", "      <td>4.029004</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6</th>\n", "      <td>6</td>\n", "      <td>Justified Season 4</td>\n", "      <td>4.009936</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>4</td>\n", "      <td>Justified Season 2</td>\n", "      <td>4.007755</td>\n", "    </tr>\n", "    <tr>\n", "      <th>47</th>\n", "      <td>47</td>\n", "      <td>Captain Phillips</td>\n", "      <td>4.000636</td>\n", "    </tr>\n", "    <tr>\n", "      <th>419</th>\n", "      <td>419</td>\n", "      <td>The Americans Season 2</td>\n", "      <td>3.997695</td>\n", "    </tr>\n", "    <tr>\n", "      <th>290</th>\n", "      <td>290</td>\n", "      <td>The Impossible</td>\n", "      <td>3.995503</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2800</th>\n", "      <td>2800</td>\n", "      <td>The Wrecking Crew</td>\n", "      <td>3.992625</td>\n", "    </tr>\n", "    <tr>\n", "      <th>19</th>\n", "      <td>19</td>\n", "      <td>Band of Brothers Season 1</td>\n", "      <td>3.991545</td>\n", "    </tr>\n", "    <tr>\n", "      <th>52</th>\n", "      <td>52</td>\n", "      <td>Maleficent (Theatrical)</td>\n", "      <td>3.991476</td>\n", "    </tr>\n", "    <tr>\n", "      <th>86</th>\n", "      <td>86</td>\n", "      <td>St. Vincent</td>\n", "      <td>3.990790</td>\n", "    </tr>\n", "    <tr>\n", "      <th>72</th>\n", "      <td>72</td>\n", "      <td>Justified Season 5</td>\n", "      <td>3.989963</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5852</th>\n", "      <td>5852</td>\n", "      <td>Inuyasha Season 1</td>\n", "      <td>3.989421</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1507</th>\n", "      <td>1507</td>\n", "      <td>Downton Abbey Season 2</td>\n", "      <td>3.989212</td>\n", "    </tr>\n", "    <tr>\n", "      <th>36</th>\n", "      <td>36</td>\n", "      <td>Downton Abbey Season 3</td>\n", "      <td>3.988906</td>\n", "    </tr>\n", "    <tr>\n", "      <th>348</th>\n", "      <td>348</td>\n", "      <td>The Ghost Army</td>\n", "      <td>3.988458</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4299</th>\n", "      <td>4299</td>\n", "      <td>Patton</td>\n", "      <td>3.987938</td>\n", "    </tr>\n", "    <tr>\n", "      <th>74</th>\n", "      <td>74</td>\n", "      <td>Downton Abbey Season 2</td>\n", "      <td>3.987414</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2709</th>\n", "      <td>2709</td>\n", "      <td>Band of Brothers Season 1</td>\n", "      <td>3.986777</td>\n", "    </tr>\n", "    <tr>\n", "      <th>563</th>\n", "      <td>563</td>\n", "      <td>Foyle's War, Series 2</td>\n", "      <td>3.986192</td>\n", "    </tr>\n", "    <tr>\n", "      <th>85</th>\n", "      <td>85</td>\n", "      <td>The Walking Dead, Season 3</td>\n", "      <td>3.984042</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1330</th>\n", "      <td>1330</td>\n", "      <td>The Walking Dead, Season 1</td>\n", "      <td>3.983070</td>\n", "    </tr>\n", "    <tr>\n", "      <th>541</th>\n", "      <td>541</td>\n", "      <td>The Big Bang Theory: The Complete Seventh Season</td>\n", "      <td>3.981733</td>\n", "    </tr>\n", "    <tr>\n", "      <th>899</th>\n", "      <td>899</td>\n", "      <td>Broadchurch Season 2</td>\n", "      <td>3.981551</td>\n", "    </tr>\n", "    <tr>\n", "      <th>238</th>\n", "      <td>238</td>\n", "      <td>Warehouse 13 Season 4</td>\n", "      <td>3.981071</td>\n", "    </tr>\n", "    <tr>\n", "      <th>81</th>\n", "      <td>81</td>\n", "      <td>The Best Exotic Marigold Hotel</td>\n", "      <td>3.979657</td>\n", "    </tr>\n", "    <tr>\n", "      <th>155</th>\n", "      <td>155</td>\n", "      <td>The Theory of Everything</td>\n", "      <td>3.979056</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3090</th>\n", "      <td>3090</td>\n", "      <td>Scrooged</td>\n", "      <td>3.978519</td>\n", "    </tr>\n", "    <tr>\n", "      <th>203</th>\n", "      <td>203</td>\n", "      <td>Masterpiece: Inspector Lewis Season 6</td>\n", "      <td>3.978420</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3131</th>\n", "      <td>3131</td>\n", "      <td>The Perfect Human Diet</td>\n", "      <td>3.446364</td>\n", "    </tr>\n", "    <tr>\n", "      <th>276</th>\n", "      <td>276</td>\n", "      <td>Under The Dome, Season 1</td>\n", "      <td>3.446108</td>\n", "    </tr>\n", "    <tr>\n", "      <th>882</th>\n", "      <td>882</td>\n", "      <td>Under the Skin</td>\n", "      <td>3.445851</td>\n", "    </tr>\n", "    <tr>\n", "      <th>32</th>\n", "      <td>32</td>\n", "      <td>Noah</td>\n", "      <td>3.445794</td>\n", "    </tr>\n", "    <tr>\n", "      <th>353</th>\n", "      <td>353</td>\n", "      <td>Noah</td>\n", "      <td>3.445701</td>\n", "    </tr>\n", "    <tr>\n", "      <th>585</th>\n", "      <td>585</td>\n", "      <td>The Watch</td>\n", "      <td>3.445020</td>\n", "    </tr>\n", "    <tr>\n", "      <th>34</th>\n", "      <td>34</td>\n", "      <td>Pilot</td>\n", "      <td>3.444534</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1611</th>\n", "      <td>1611</td>\n", "      <td>Snow White &amp; The Huntsman</td>\n", "      <td>3.443367</td>\n", "    </tr>\n", "    <tr>\n", "      <th>29</th>\n", "      <td>29</td>\n", "      <td>Noah</td>\n", "      <td>3.442196</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5124</th>\n", "      <td>5124</td>\n", "      <td>The Big Wedding</td>\n", "      <td>3.441997</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1171</th>\n", "      <td>1171</td>\n", "      <td>Anna Karenina</td>\n", "      <td>3.441532</td>\n", "    </tr>\n", "    <tr>\n", "      <th>834</th>\n", "      <td>834</td>\n", "      <td>Anchorman 2: The Legend Continues</td>\n", "      <td>3.439384</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1322</th>\n", "      <td>1322</td>\n", "      <td>The Man With The Iron Fists</td>\n", "      <td>3.439276</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2357</th>\n", "      <td>2357</td>\n", "      <td>Monsters</td>\n", "      <td>3.438530</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2</td>\n", "      <td>Extant, Season 1</td>\n", "      <td>3.437644</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1024</th>\n", "      <td>1024</td>\n", "      <td>Seven Psychopaths</td>\n", "      <td>3.436573</td>\n", "    </tr>\n", "    <tr>\n", "      <th>579</th>\n", "      <td>579</td>\n", "      <td>Summer in February</td>\n", "      <td>3.436331</td>\n", "    </tr>\n", "    <tr>\n", "      <th>240</th>\n", "      <td>240</td>\n", "      <td>Abraham Lincoln: Vampire Hunter</td>\n", "      <td>3.435600</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1061</th>\n", "      <td>1061</td>\n", "      <td>Jackass Presents: Bad Grandpa - Extended</td>\n", "      <td>3.435299</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4430</th>\n", "      <td>4430</td>\n", "      <td>Pilot</td>\n", "      <td>3.435126</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2169</th>\n", "      <td>2169</td>\n", "      <td>Eden Log</td>\n", "      <td>3.434194</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1306</th>\n", "      <td>1306</td>\n", "      <td>Dominion, Season 1</td>\n", "      <td>3.433816</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1112</th>\n", "      <td>1112</td>\n", "      <td>Chappie</td>\n", "      <td>3.428702</td>\n", "    </tr>\n", "    <tr>\n", "      <th>211</th>\n", "      <td>211</td>\n", "      <td>Hercules (2014 Extended Cut)</td>\n", "      <td>3.428078</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1365</th>\n", "      <td>1365</td>\n", "      <td>The Woman in Black 2: Angel of Death</td>\n", "      <td>3.427136</td>\n", "    </tr>\n", "    <tr>\n", "      <th>13</th>\n", "      <td>13</td>\n", "      <td>Under The Dome, Season 2</td>\n", "      <td>3.426847</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3623</th>\n", "      <td>3623</td>\n", "      <td>Third Star</td>\n", "      <td>3.425323</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2821</th>\n", "      <td>2821</td>\n", "      <td>A Good Day to Die Hard</td>\n", "      <td>3.424551</td>\n", "    </tr>\n", "    <tr>\n", "      <th>781</th>\n", "      <td>781</td>\n", "      <td>Inside Amy Schumer Season 1</td>\n", "      <td>3.421530</td>\n", "    </tr>\n", "    <tr>\n", "      <th>571</th>\n", "      <td>571</td>\n", "      <td>Transparent Season 1</td>\n", "      <td>3.421061</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>6659 rows × 3 columns</p>\n", "</div>"], "text/plain": ["      item                                     product_title     score\n", "25      25                               John Adams Season 1  4.073676\n", "5        5                            Downton Abbey Season 3  4.051294\n", "12      12                                Justified Season 3  4.029004\n", "6        6                                Justified Season 4  4.009936\n", "4        4                                Justified Season 2  4.007755\n", "47      47                                  Captain Phillips  4.000636\n", "419    419                            The Americans Season 2  3.997695\n", "290    290                                    The Impossible  3.995503\n", "2800  2800                                 The Wrecking Crew  3.992625\n", "19      19                         Band of Brothers Season 1  3.991545\n", "52      52                           Maleficent (Theatrical)  3.991476\n", "86      86                                       St. Vincent  3.990790\n", "72      72                                Justified Season 5  3.989963\n", "5852  5852                                 Inuyasha Season 1  3.989421\n", "1507  1507                            Downton Abbey Season 2  3.989212\n", "36      36                            Downton Abbey Season 3  3.988906\n", "348    348                                    The Ghost Army  3.988458\n", "4299  4299                                            Patton  3.987938\n", "74      74                            Downton Abbey Season 2  3.987414\n", "2709  2709                         Band of Brothers Season 1  3.986777\n", "563    563                             Foyle's War, Series 2  3.986192\n", "85      85                        The Walking Dead, Season 3  3.984042\n", "1330  1330                        The Walking Dead, Season 1  3.983070\n", "541    541  The Big Bang Theory: The Complete Seventh Season  3.981733\n", "899    899                              Broadchurch Season 2  3.981551\n", "238    238                             Warehouse 13 Season 4  3.981071\n", "81      81                    The Best Exotic Marigold Hotel  3.979657\n", "155    155                          The Theory of Everything  3.979056\n", "3090  3090                                          Scrooged  3.978519\n", "203    203             Masterpiece: Inspector Lewis Season 6  3.978420\n", "...    ...                                               ...       ...\n", "3131  3131                            The Perfect Human Diet  3.446364\n", "276    276                          Under The Dome, Season 1  3.446108\n", "882    882                                    Under the Skin  3.445851\n", "32      32                                              Noah  3.445794\n", "353    353                                              Noah  3.445701\n", "585    585                                         The Watch  3.445020\n", "34      34                                             Pilot  3.444534\n", "1611  1611                         Snow White & The Huntsman  3.443367\n", "29      29                                              Noah  3.442196\n", "5124  5124                                   The Big Wedding  3.441997\n", "1171  1171                                     Anna Karenina  3.441532\n", "834    834                 Anchorman 2: The Legend Continues  3.439384\n", "1322  1322                       The Man With The Iron Fists  3.439276\n", "2357  2357                                          Monsters  3.438530\n", "2        2                                  Extant, Season 1  3.437644\n", "1024  1024                                 Seven Psychopaths  3.436573\n", "579    579                                Summer in February  3.436331\n", "240    240                   Abraham Lincoln: Vampire Hunter  3.435600\n", "1061  1061          Jackass Presents: Bad Grandpa - Extended  3.435299\n", "4430  4430                                             Pilot  3.435126\n", "2169  2169                                          Eden Log  3.434194\n", "1306  1306                                Dominion, Season 1  3.433816\n", "1112  1112                                           Chappie  3.428702\n", "211    211                      Hercules (2014 Extended Cut)  3.428078\n", "1365  1365              The Woman in Black 2: Angel of Death  3.427136\n", "13      13                          Under The Dome, Season 2  3.426847\n", "3623  3623                                        Third Star  3.425323\n", "2821  2821                            A Good Day to Die Hard  3.424551\n", "781    781                       Inside Amy Schumer Season 1  3.421530\n", "571    571                              Transparent Season 1  3.421061\n", "\n", "[6659 rows x 3 columns]"]}, "execution_count": 45, "metadata": {}, "output_type": "execute_result"}], "source": ["titles.sort_values(['score', 'item'], ascending=[False, True])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** What can you conclude from the highly rated and lowest rated shows for the user?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** The predicted highly rated shows have some well-reviewed TV dramas and some book adaptations. The lowest scores are from comedies, child, and teenage movies.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["See if your recommendations have correlations with other users. Try user 201. Perform the same operations as you did for user 200."]}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [], "source": ["user_201 = prepare_predictions(201, products.shape[0], customers.shape[0] + products.shape[0])\n", "\n", "pred_201 = []\n", "for i in range(0, user_201.shape[0], 5):\n", "    preds = fm_predictor.predict(user_201[i:i+5].toarray())['predictions']\n", "    p = [pred_201.append(x['score']) for x in preds]"]}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"data": {"text/plain": ["<matplotlib.collections.PathCollection at 0x7f01bf4cbdd8>"]}, "execution_count": 49, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHfZJREFUeJzt3X9s3PWd5/Hn28NAxiyLU+KuiMGkWlrY8svu+rjcId2W9PixUNIQYANXpCL1yOl+tLuFzYrsRSSkZSnKCTipnO5Ytlq2/GggS0chvV6KjlSoiKQ4N/lBKOG6XQgdVoJCjBRico7zvj/mO2YynvF87cx8f83rIVnMjD/xvGWZlz/+fD/f98fcHRERyZaeuAsQEZH2U7iLiGSQwl1EJIMU7iIiGaRwFxHJIIW7iEgGKdxFRDJI4S4ikkEKdxGRDDoprjdesGCBL1q0KK63FxFJpZ07d/7W3ftbjYst3BctWsTo6Ghcby8ikkpm9laYcVqWERHJIIW7iEgGKdxFRDJI4S4ikkEKdxGRDFK4i4hkkMJdRCSDFO4iIhmkcBcRyaDY7lAVEUmjYqnMhq37eWdsnIV9BVZddR7LhgfiLmsahbuISEjFUpnVz+5lfGISgPLYOKuf3QuQuIDXsoyISEgbtu6fCvaq8YlJNmzdH1NFzSncRURCemdsfFavxyl0uJtZzsxKZralwedOMbONZvYrM9thZovaWaSISBIs7CvM6vU4zWbm/qfAL5t87uvAQXc/F3gQuP9ECxMRSZpVV51HIZ877rVCPseqq86LqaLmQoW7mZ0FXAs82mTIV4DHgsebgC+ZmZ14eSIiybFseID7ll/EQF8BAwb6Cty3/KLEXUyF8LtlHgL+AjityecHgLcB3P2omX0InAH8tnaQma0EVgIMDg7OpV4RkVgtGx4IHeZxbptsOXM3sy8D77r7zhN9M3d/xN1H3H2kv7/lKVEiIqlV3TZZHhvH+WTbZLFUjuT9wyzLXAYsNbM3gR8CS8zs8boxZeBsADM7CTgdeL+NdYqIpErc2yZbhru7r3b3s9x9EXAz8IK731o3bDPwteDxjcEYb2ulIiIpEve2yTnvczez9Wa2NHj6N8AZZvYr4A7grnYUJyKSVnFvm5xVuLv7z9z9y8Hju919c/D4Y3e/yd3PdfdL3f3XnShWRCQt4t42qd4yIpIJSWvoVX3vuGpSuItIYjUKbJgemEAiG3rNZttku1lc1z1HRkZ8dHQ0lvcWkeSr78AIkM8ZOEwc+yS3DGiWYgN9BV66a0lnC42Yme1095FW4zRzF5FEarSVcGJyeozPND1NYkOvqCjcRSRW1aWX8tg4OTMm3Znfm+fg4YkT/tpJbOgVFYW7iMSmfullMlgmbkewA3x05CjFUjnUunfSLsieKIW7iMTmnuf2TVt6aaex8QlWP7uX0bc+YMvuf2JsvPJLY35vnrXXXTAV3mk6YSksHdYhIrEolsptm6HPZHxikse3H5gKdqj8ZbBq0+6pPi9xtwroBM3cRSRStWvscZqYdDZs3c+y4YHYWwV0gsJdRCJRLJX5y2f3cHjiWNylTKmG98K+QsNfNmm+IKtlGRHpuGKpzB1P70pUsMMn4R13q4BO0MxdRNqufufJwY+OcCxhfWLzOZsK77hbBXSCwl1E2mpNcS9PbD8wdXNR3GvrjdTvloF4WwV0gsJdRE5I7Sz99EL+uF0pSZLFVgQzUbiLyJzV7w9ParCnff18LhTuIjJnjfaHJ0XOjGPumVg/nwuFu4jMWVL3gRfyOe5bflHXBXothbuINDVTv5ViqUxP0Ograbo92EHhLiJNNOq38q2Nuxh96wNGzvkUq5/dm8hgH+grdH2wg8JdRBoolsrc+fTuaeHtwOPbD/D49gPxFNZCN144bUbhLiLHKZbKrHpmerAn3UCXXjhtpmW4m9k84EXglGD8JndfWzfmHOD7QD/wAXCru/+m/eWKSKet27zvuGPskq7b9q+HFaa3zBFgibtfAgwBV5vZ4rox/wX4O3e/GFgP3NfeMkUkKkndq95Ivse0DNNEy3D3ikPB03zwUf9r/fPAC8HjbcBX2lahiESiWCpz2XdfaD0wIfoKeTbcdImWYZoIteZuZjlgJ3Au8LC776gbshtYDvxX4HrgNDM7w93fr/s6K4GVAIODgydYuoi0y5ri3sReJK2lJZjwQoW7u08CQ2bWB/zIzC5091drhvw58D0zu43K+nwZmHbbmrs/AjwCMDIykp5FPZEMWlPcy1M73k7NhVPthJmdWe2WcfcxM9sGXA28WvP6O1Rm7pjZ7wA3uPtYOwsVkdmZ6QakpM/UDfiXv/8p3nx/PDMteKMWZrdMPzARBHsBuAK4v27MAuADdz8GrKayc0ZEYtLqwOendrwdZ3kz0pbG9gizW+ZMYJuZ7QFeAZ539y1mtt7MlgZjvgjsN7M3gN8D7u1ItSISSqsDn5O8FPPSXUsU7G3Qcubu7nuA4Qav313zeBOwqb2lichcpfXA54EUn1maNDpDVSSDmh3snOQDn3XBtL0U7iIZ1OzA58vP72d4/U9jqup483vzDPQVMCozdnVybC/1lhHJqHn5nql1d7PKmnuSdsiMHZ6gdPeVcZeRWQp3kYwolsrc89w+Dh6e3j4giddPk7xElAUKd5EMKJbKrNq0m4nJBKZ4A1pf7zytuYtkwIat+xMX7Lke49bFg1M7YHJmgNbXo6KZu0hKVe9ALSdwe+P83jxrr7tAAR4jhbtIyhRLZdZt3pfI1rwPrRhSoCeEwl0kJdYU9/LkjgMk9RwNnV2aLAp3kRRIeqMvXSBNHoW7SAokrdHX/N487vDh+IQ6NiaUwl0kgWrb9RbyPYlp9GXAg1pXTwWFu0jC1LfrPTxxLOaKPuGgYE8J7XMXSZh1m/dNa9ebFOramB6auYskRGXGvofxBM3Ua+miaboo3EUSoFgqs+qZ3UwkbJ9jzoxj7rpomkIKd5EE2LB1f+KCvZDPqU1AiincRSLU6NBqIHEtBHJmCvaUU7iLRKTRodV/tnFXzFU1dsxdwZ5y2i0jEpFGh1YnlXqtp1/LcDezeWb2CzPbbWb7zOyeBmMGzWybmZXMbI+ZXdOZckXSqVgqJ27ppcrqnmtXTDaEmbkfAZa4+yXAEHC1mS2uG7MGeNrdh4Gbgf/W3jJF0qu6HJMk83vzPLRiiDe/ey0PrhjSWaYZ1HLN3d0dOBQ8zQcf9Zf1Hfjd4PHpwDvtKlAkqRpdHG0Uiklcjuk9+aSpWpcNDyjMMyjUBVUzywE7gXOBh919R92QdcBPzewbwKnAv25nkSJJ0+jiaHV2vmx44LjgT9YGx4p3ErpEJO0T6oKqu0+6+xBwFnCpmV1YN+QW4G/d/SzgGuAHZjbta5vZSjMbNbPR995770RrF4lNo9n4+MQkG7bunwr+ckKDHXTBtBvMareMu48B24Cr6z71deDpYMzLwDxgQYN//4i7j7j7SH9//9wqFkmAZjPfd8bGE7kMU0sXTLtDmN0y/WbWFzwuAFcAr9cNOwB8KRjzB1TCXVNzyaxmM18neTck1dIF0+4RZuZ+JrDNzPYArwDPu/sWM1tvZkuDMXcCt5vZbuAp4LbgQqxIJq266jwK+VzcZUzTm++hr5Bv+LmBvgIv3bVEwd4lwuyW2QMMN3j97prHrwGXtbc0keRaNjzA6Fsf8MT2A4lYV8+Zccs/P5vvLLto2sVe0FJMN1L7AZE5KJbKPLXj7ViD/dSTc9x7/fQllurzMNs0JbsU7iKzVJ0Zx3X0nQFfXTzId5Zd1HSM9q6Lwl1kluLaDXPZ73+KJ27/F5G/r6STGoeJzFJcNwAp2GU2NHMXCal612kcizHzexvvgBFpRuEuEkKjHShRyeeMtdddEPn7Srop3EVaKJbK3Pn07kguoObMmHSf+u+AdrrIHCncRWpUl17KY+MY09ufdorOK5V2U7iLBNYU9x53U1JUwa7ZuXSCwl26WrFU5p7n9nHw8ESk71vI93Df8osV6NIxCnfpWsVSmTue3sWxCLe/VPu7iHSawl0yq9VJSes274s02PM9pv4uEhmFu2RSq5OSAMbGo1uK6SvkWbf0Ai3DSGQU7pJJM52UBJVZe1QMFOwSObUfkExq1iKgOoOPctbuMPVLRSQqmrlL6jVaW1/YV2h4IlLOLJa7TJN8OpNkk2bukmr1h1FXZ+aXn9/f8KSkuNr05sxieV/pXgp3SbV1m/c1XFt/fPsBTjmph1NP7txReA+tGGKgyVmq9eL6pSLdS8syklrFUnnGtfNOr6tXl4LCCPtLQKRdNHOX1Ir7ImV1jb8VnV8qcVC4S2rFfZFyYV+By8/vp341Pd9jzO/NY1Rm7GoIJnFouSxjZvOAF4FTgvGb3H1t3ZgHgcuDp73Ap929r821ikwplsqRdm2sl+8xLj+/n7/fWT6uBgNWXHr2jOebikQhzJr7EWCJux8yszzwczP7ibtvrw5w929VH5vZN4Dh9pcq3ax+u+NHR47GFuwGbLjpkoY3Sjmw7fX3YqlLpFbLZRmvOBQ8zQcfM/1/dQvwVBtqEwEab3eM8iakWoV8jgdXDLFseKDpxdS4zlgVqRVqzd3Mcma2C3gXeN7ddzQZdw7wGeCF9pUo3a7RDDkqPVZpz9to/bzZxdQwF1lFOi3UVkh3nwSGzKwP+JGZXejurzYYejOVNfmG/yea2UpgJcDg4OAcS5ZuE9dMOGfGP9x3TdPPr7rqvGnnqmpnjCTFrPa5u/uYmW0Drgaahft/nOHfPwI8AjAyMqK7OmTKTO15C/keDk8ci7ymVjceVeubqa2wSFzC7JbpByaCYC8AVwD3Nxh3PjAfeLntVUqmtWrPO340+mCHcDceLRseUJhLIoWZuZ8JPGZmOSpr9E+7+xYzWw+MuvvmYNzNwA/ddZ+1zE6r9rxx/ERpeUXSzuLK4pGRER8dHY3lvSVZPnPXj2Pb1tjI/N48a69T/3VJJjPb6e4jrcapt4zErll73qgNaM1cMkThLrGovYB6eiFPrseYjPJA0wZ0cLVkicJdIld/ATWuG5JEskyNwyRycd6U1ExfIR93CSJtpXCXyMW5vj6/Nz/thz7fY6xbekEs9Yh0isJdIlMslRle/9NY3ruQz/HQiiFKd1/JA8EJStWWAhtuukQXUSVzFO4Sieo6+8HDnV9f7wFuXTx4XICrp7p0G11QlbZr1EogqnV2Ax4IujY2q22mu2FFskLhLm21priXJ7YfmLopqTw2zh0bdxFVA4GvLh6cMaRnuhtW4S5ZomUZaZtiqXxcsFdF2Rmm1UEZ6sEu3ULhLm2zbvO+2NsItApp9WCXbqFwl7ZYU9ybiJuRWoX0qqvOo5DPHfeamoRJFmnNXU5IsVRm3eZ9kQT7KSf1UMjnmr5XmJBWD3bpFgp3mbP6nSedlM8Z999wMcuGB6Z245THxsmZMek+q6Zf6sEu3UDhLrO2priXp3a83fKkonapD26Fs0hrCndpqXbfeu/JOT76f9H2hVG3RpHZU7jLjOqXXqIO9jBH3YnIdNotIzOKs4OjdrGIzJ1m7jKjuDo46qg7kROjcJemvvrXL3f8PU49Ocf1Xxhg2+vvaWuiSBsp3AWg4fbCTusr5Nm19sqOv49IN2oZ7mY2D3gROCUYv8nd1zYY9yfAOsCB3e7+b9pbqnRKfbOvKII9n9MBGSKdFGbmfgRY4u6HzCwP/NzMfuLu26sDzOyzwGrgMnc/aGaf7lC90mbFUpnHtx+I9D21ni7SeS3D3d0dOBQ8zQcf9VO724GH3f1g8G/ebWeR0n61yzBRyffA//2rayN7P5FuFmrN3cxywE7gXCohvqNuyOeCcS8BOWCdu/+vdhYqJ6b2RqSTemAiyj68gaMxvKdItwoV7u4+CQyZWR/wIzO70N1frfs6nwW+CJwFvGhmF7n7WO3XMbOVwEqAwcHBNpQvzdSGeV9vnkMfH2XiWOUPrjiCHdRWVyRKs7qJKQjrbcDVdZ/6DbDZ3Sfc/R+BN6iEff2/f8TdR9x9pL+/f641SwvVu0rLY+M4cPDwxFSwx8VANySJRKhluJtZfzBjx8wKwBXA63XDilRm7ZjZAirLNL9ua6USWpx3lTbj6IxSkSiFmbmfCWwzsz3AK8Dz7r7FzNab2dJgzFbgfTN7jcrMfpW7v9+ZkqWVOI+M6yvkG76uHjEi0QqzW2YPMNzg9btrHjtwR/AhMVvYV4i8bUAP8MCKIYBpPd7VI0YkerpDNcVqL5pWb9sH+OCjI5HWkTPjH+675rjXdNKRSLwU7ilV34q3PDbOn23cFUst9Xe06jANkfip5W9KJemiqdbTRZJHM/cEa7TsUp0Rx3nRtJbW00WSSeGeUI2WXVY/uxeAZ0YPTOv/EJf7ll+kJRiRBFK4J1SjZZfxicnY1tUbGegrKNhFEkpr7gmVlGWXZrQcI5JsCveESkoflkK+h/m9lRuTcmZAZcau5RiRZNOyTEIdnYx/J4xOShJJL4V7AtTuijm9kOfD8YnYL5gW8jmdlCSSYgr3mNXvihkbn4i5osqyi+4qFUk3hXvM1m3el4ibkQr5nNbRRTJE4R6DOI64m0nOTMEukjEK94itKe7lie3JuQkpnzM23HiJgl0kYxTuEUjaTL3q1JNz3Hu9ZuwiWaRw77D6C6ZJYMCDK4YU6iIZppuYOixJ3RurFOwi2adw77CktRHozfco2EW6gMK9g4qlctwlTPNXyy+OuwQRiYDCvUOqa+1J2RVTpVm7SHfQBdU2S+rOGNCJSSLdpGW4m9k84EXglGD8JndfWzfmNmADUF2H+J67P9reUpOj2cHU6zbvS0T7gEbUoleku4SZuR8Blrj7ITPLAz83s5+4+/a6cRvd/T+1v8RkaXRC0h0bd3Es5rpmol4xIt2nZbi7uwOHgqf54CNpS8mRabS1ManBrn4xIt0r1AVVM8uZ2S7gXeB5d9/RYNgNZrbHzDaZ2dltrTJBkra1sdbJOWOgr4ChAzVEul2oC6ruPgkMmVkf8CMzu9DdX60Z8hzwlLsfMbN/BzwGLKn/Oma2ElgJMDg4eMLFR61YKtNjxqTH+4fL/N48hz4+ysSxT+rQLF1Eas1qK6S7jwHbgKvrXn/f3Y8ETx8F/rDJv3/E3UfcfaS/v38u9camutYed7A/tGKI0t1XsuGmSzRLF5GmwuyW6Qcm3H3MzArAFcD9dWPOdPd/Cp4uBX7Z9kpjsKa4lyd3HOBYQq4w9BXyUwG+bHhAYS4iTYVZljkTeMzMclRm+k+7+xYzWw+Muvtm4JtmthQ4CnwA3NapgqOypriXx7cfiLuMKTr2TkRmI8xumT3AcIPX7655vBpY3d7S4vXkjuQEu7Yyishs6Q5VGt+UlJSlGICX7pp2bVpEZEZdHe7FUpl7ntvHwcOf3FVaHhtn1TO7Y6zqeGoZICJz0bXhXiyVWfXM7uO2E1Y1ei0OahkgInPVteG+bvO+xIR4LQv+u1Dr7CJyAjIb7o3W0WuDMokNvnT8nYi0SybDvVFzr29t3MXoWx8wcs6n2LB1f8wVNuao37qItEcmw71Rcy8HHt9+IBF713NNWhjo4qmItEvqw73R8kuSm3v1FfKsW3rBcX9ZgC6eikh7pTrc1xT38sT2A1P9h8tj46x+di+nF/KJXFMH+HB8YmrpZaZrAiIiJyK14V4slY8L9qrxiUkswe3mFwZLL+oNIyKdlNoDsjds3d80wg9PJPP4DC29iEhUUjtzT/K6ei2jcjFX/WFEJEqpDfeFfQXKCQ/46sVTBbqIRC21yzKrrjqPQj4XdxlTzCphXj0846EVQ+xae6WCXURikdqZezU0123el4ydMQ671l4ZdxUiIkCKZ+5QCfhTT0nG76eFugFJRBIk1eEOybiwql0wIpI0qQ/3vt58LO+bM9Ph1CKSWMlY0zgBDVq0dFw+Z2y48RIFuogkVupn7h9GfDG1N9+jYBeRxEv9zD2q/e6nnpzj3uu1/CIi6dBy5m5m88zsF2a228z2mdk9M4y9wczczEbaW2Zzq646j3zOWg+co+qe9X3rr1awi0hqhJm5HwGWuPshM8sDPzezn7j79tpBZnYa8KfAjg7U2VQ1cOsPup6tapuAqkI+pwulIpJaLWfuXnEoeJoPPhpdxvw2cD/wcfvKC2fZ8AClu6/kze9ey0MrhuiZ5UR+fm+eB1cMMdBX0A4YEcmEUGvuZpYDdgLnAg+7+466z38BONvdf2xmq2b4OiuBlQCDg4NzLnom1UD+y2f3hO4Oufa6C9SCV0QyJdRuGXefdPch4CzgUjO7sPo5M+sBHgDuDPF1HnH3EXcf6e/vn2vNLS0bHuC1b/9xqLG3Lh5UqItI5sxqK6S7jwHbgKtrXj4NuBD4mZm9CSwGNkd5UbWZMGeSfmfZRRFUIiISrTC7ZfrNrC94XACuAF6vft7dP3T3Be6+yN0XAduBpe4+2qGaQ2vVOVIHUotIVoWZuZ8JbDOzPcArwPPuvsXM1pvZ0s6Wd2KWDQ9w3/KLmN+kRcFHR45SLJUjrkpEpPPM47h/HxgZGfHR0egm98VSueF2SW15FJE0MbOd7t5y2Tv17QfCWjY8QO/J0zcHjU9MsmHr/hgqEhHpnK4Jd2jeHjgJbYNFRNqpq8K92YEaOmhDRLKmq8K90e4ZHbQhIlmU+q6Qs1G9aLph637eGRtnYV+BVVedp4upIpI5XRXugNoMiEhX6KplGRGRbqFwFxHJIIW7iEgGKdxFRDJI4S4ikkEKdxGRDFK4i4hkkMJdRCSDFO4iIhkUWz93M3sPeCvE0AXAbztcTrulsWZIZ92qOTpprDuNNcPMdZ/j7i0PoY4t3MMys9EwjemTJI01QzrrVs3RSWPdaawZ2lO3lmVERDJI4S4ikkFpCPdH4i5gDtJYM6SzbtUcnTTWncaaoQ11J37NXUREZi8NM3cREZmlRIS7mc0zs1+Y2W4z22dm98ww9gYzczOL9Qp4mJrN7DYze8/MdgUf/zaOWmvqCfV9NrM/MbPXgjFPRl1ng3rCfK8frPk+v2FmY3HUWlNPmJoHzWybmZXMbI+ZXRNHrXU1han7HDP730HNPzOzs+KotZ6Z5YLv5ZYGnzvFzDaa2a/MbIeZLYq+wula1PyvzOz/mNlRM7tx1l/c3WP/AAz4neBxHtgBLG4w7jTgRWA7MJL0moHbgO/F/f2dZc2fBUrA/OD5p9NQd934bwDfT3rNVNZV/33w+PPAm2n4XgPPAF8LHi8BfhB33UEtdwBPAlsafO4/AP89eHwzsDHuekPUvAi4GPg74MbZfu1EzNy94lDwNB98NLoY8G3gfuDjqGprZhY1J0bImm8HHnb3g8G/eTfCEhuaw/f6FuCpjhc2g5A1O/C7wePTgXciKq+pkHV/HngheLwN+EpE5TUV/PVwLfBokyFfAR4LHm8CvmRmFkVtzbSq2d3fdPc9wLG5fP1EhDtM/XmyC3gXeN7dd9R9/gvA2e7+41gKbKBVzYEbgj9fN5nZ2RGXOE2Imj8HfM7MXjKz7WZ2dfRVThfye42ZnQN8hk/CJzYhal4H3GpmvwH+J5W/OGIXou7dwPLg8fXAaWZ2RpQ1NvAQ8Bc0D8IB4G0Adz8KfAgkveYTkphwd/dJdx8CzgIuNbMLq58zsx7gAeDOuOprZKaaA88Bi9z9YuB5Ppk5xCZEzSdRWZr5IpUZ8F+bWV+0VU4Xou6qm4FN7j4ZXXWNhaj5FuBv3f0s4BrgB8HPeqxC1P3nwB+ZWQn4I6AMxPb9NrMvA++6+864apitKGqO/QepnruPUflTr3bGeBpwIfAzM3sTWAxsjvuialWTmnH39939SPD0UeAPo66tmWY1A78BNrv7hLv/I/AGlbBPhBnqrrqZmJdk6s1Q89eBp4MxLwPzqPQUSYQZfq7fcffl7j4M/OeasXG5DFgaZMMPgSVm9njdmDJwNoCZnURlGez9KIusE6bmE5KIcDez/urs0MwKwBXA69XPu/uH7r7A3Re5+yIqF1SXuvtoLAXTuubg9TNrni4FfhldhdOFqRkoUpm1Y2YLqCzT/DrCMqcJWTdmdj4wH3g52gqnC1nzAeBLwZg/oBLu70VZZ72QP9cLav7CWA18P9oqj+fuq939rCAbbgZecPdb64ZtBr4WPL4xGBPbNbKQNZ+QRIQ7cCawzcz2AK9QWefbYmbrzWxpzLU1E6bmbwbbyXYD36SyeyZOYWreCrxvZq9RmbWtcvc4ZzgQ/ufjZuCHcf5PWyNMzXcCtwc/H08BtyWg9jB1fxHYb2ZvAL8H3BtPqTOrq/lvgDPM7FdUdqjcFV9lzdXWbGb/LLgecxPwP8xs36y+Vvw/SyIi0m5JmbmLiEgbKdxFRDJI4S4ikkEKdxGRDFK4i4hkkMJdRCSDFO4iIhmkcBcRyaD/D8o/0eU4DRHEAAAAAElFTkSuQmCC\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["plt.scatter(pred_200, pred_201)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Question:** What can you conclude from the scatter plot between the two users?  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Answer:** This correlation is nearly perfect. Essentially, the average rating of items dominates across users, and our system will recommend the same well-reviewed items to everyone. That basically means that the model is only finding the best rated shows and presenting them to all users. Now we need to see if we can do better by shifting the paradigm from regression to classification."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Delete the endpoint you created for inference because you won't be using it anymore."]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": ["sagemaker.Session().delete_endpoint(fm_predictor.endpoint)"]}], "metadata": {"kernelspec": {"display_name": "conda_mxnet_p36", "language": "python", "name": "conda_mxnet_p36"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat": 4, "nbformat_minor": 4}